{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "g20_teacher_net_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9kk2RUNv_iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "64958964-de68-46b0-c5ec-8cb10e55188a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/My Drive/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPJVqAKyml5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30ef6142-23c9-4923-f9c2-fc986df3c1f7"
      },
      "source": [
        "VERSION = \"20200516\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4139  100  4139    0     0  53753      0 --:--:-- --:--:-- --:--:-- 53753\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200516 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (47.3.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.10.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Uninstalling torch-1.5.1+cu101:\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.9)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.5.1+cu101\n",
            "Uninstalling torchvision-0.6.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n",
            "Operation completed over 1 objects/91.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "| [1 files][119.8 MiB/119.8 MiB]                                                \n",
            "Operation completed over 1 objects/119.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200516-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
            "Operation completed over 1 objects/2.3 MiB.                                      \n",
            "Processing ./torch-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200516) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200516) (1.18.5)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.6.0a0+83df3be\n",
            "Processing ./torch_xla-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2191422\n",
            "Processing ./torchvision-nightly+20200516-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (1.6.0a0+83df3be)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200516) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200516) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.7.0a0+348dd5a\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (379 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MazDUu-ojrvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "df9b4aa1-fc15-4e39-8def-1ed5c6a02e98"
      },
      "source": [
        "%cd Lightweighted/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1ri6ldikC2f57UGoFJbYCFGVeHLB2yECf/Lightweighted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njUvtjGO4qcm",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9qV4MaefQk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Result Visualization Helper\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/test_result.png'\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize((-0.1307/0.3081,), (1/0.3081,))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(11, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.squeeze() # [1,Y,X] -> [Y,X]\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(label, prediction), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnZP5Ph4svv",
        "colab_type": "text"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzaetGN2jy6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from gtsrb_dataset import GTSRB\n",
        "from torch.autograd import Variable as var\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhJ1qJSFzN0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses_df = pd.DataFrame(columns=['Epoch', 'Loss', 'Accuracy'])\n",
        "logging.basicConfig(filename='./training_teacher_tpu.log', filemode='w', format='%(levelname)s - %(message)s')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhCyJXiu1vHZ",
        "colab_type": "text"
      },
      "source": [
        "## Save Model State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjXHikSuZrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveModel(epoch, model, optimizer, loss, path):\n",
        "  torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': loss\n",
        "              }, path)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37Tk_o11yIO",
        "colab_type": "text"
      },
      "source": [
        "## Load Model State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72pnhO3N1zyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadModel(model, path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    print('Epoch: ',epoch,'Loss: ',loss)\n",
        "    return model, epoch, loss;"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg6HyJNp_tcx",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Hyper-parameters for Teacher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eWzqwMiwV54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PARAMETERS = {}\n",
        "PARAMETERS['saved_models'] = \"saved_models/teacher_model_tpu_z.pth\"\n",
        "PARAMETERS['learning_rate'] = 0.001\n",
        "PARAMETERS['epochs'] = 300\n",
        "PARAMETERS['weight_decay'] = 0.0001\n",
        "PARAMETERS['batch_size'] = 128 \n",
        "PARAMETERS['growth_rate'] = 128 ## Growth rate and batch size\n",
        "PARAMETERS['num_workers'] = 4\n",
        "PARAMETERS['num_cores'] = 8\n",
        "PARAMETERS['log_steps'] = 20\n",
        "PARAMETERS['load_from_saved'] = False\n",
        "PARAMETERS['start_epoch'] = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwPdDq8Zvrkx",
        "colab_type": "text"
      },
      "source": [
        "## Loading GTSRB Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bmN4bcPvuY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.3403, 0.3121, 0.3214),\n",
        "                         (0.2724, 0.2608, 0.2669))\n",
        "])\n",
        "\n",
        "train_data = GTSRB(\n",
        "    root_dir = './data/', train=True,  transform=transform)\n",
        "test_data = GTSRB(\n",
        "    root_dir = './data/', train=False,  transform=transform)\n",
        "\n",
        "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_data,\n",
        "    num_replicas = xm.xrt_world_size(),\n",
        "    rank = xm.get_ordinal(),\n",
        "    shuffle = True)\n",
        "\n",
        "train_set = torch.utils.data.DataLoader(\n",
        "    train_data, \n",
        "    batch_size = PARAMETERS['batch_size'],\n",
        "    sampler = train_sampler,\n",
        "    num_workers = PARAMETERS['num_workers'], \n",
        "    pin_memory = True)\n",
        "\n",
        "test_set = torch.utils.data.DataLoader(\n",
        "    test_data, \n",
        "    batch_size = PARAMETERS['batch_size'],\n",
        "    shuffle = False,\n",
        "    num_workers = PARAMETERS['num_workers'],\n",
        "    pin_memory = True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFBZ92U75MuD",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Teacher Network\n",
        "The following sub-sections define the various parts of the Teacher Network.\n",
        "We start by defining the Cell block, followed by the Stage module and finally the complete teacher model as defined in the paper \\\\\n",
        "[Lightweight deep network for traffic sign classification, Zhang et. al (2019)](https://rdcu.be/b5aTv)\n",
        "\n",
        "Before coding the network, we show the visual description of how the network looks with images taken from the above paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsCyzdxZkGUS",
        "colab_type": "text"
      },
      "source": [
        "## Cell Block\n",
        "\n",
        "*The 1 × 1 kernels and the 3 × 3 kernels execute convolution\n",
        "operations in parallel and splice all output results*\n",
        "\n",
        "![Cell Block](https://i.imgur.com/RWMjelN.png)\n",
        "\n",
        "Please note, the numbers 64 on each on the convolution operations are, as per our interpretation, used to denote that each of the convolution operations see exactly half of the input(as the batch size mentioned in the paper is 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8zB6K6AwptD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cell(nn.Module):\n",
        "  def __init__(self,cell_in_channels,cell_out_channels):\n",
        "    super(Cell, self).__init__()\n",
        "\n",
        "    self.activation_function = nn.ReLU()\n",
        "    self.batch_norm = nn.BatchNorm2d(cell_out_channels)\n",
        "\n",
        "    ## Reflect padding is used for the 3 x 3 convolution as it creates a \n",
        "    ## feature map of size 30 X 30, and needs to be padded to 32 x 32\n",
        "    ## in order to concatenate with the 1 x 1 conv tensor\n",
        "\n",
        "    self.cnn3 = nn.Conv2d(in_channels=int(cell_in_channels/2),\n",
        "                          out_channels=int(cell_out_channels/2),\n",
        "                          kernel_size=3,padding=1,padding_mode='reflect', \n",
        "                          stride=1)\n",
        "    \n",
        "    self.cnn1 = nn.Conv2d(in_channels=int(cell_in_channels/2), \n",
        "                          out_channels=int(cell_out_channels/2),\n",
        "                          kernel_size=1, stride=1)\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    I had initially thought about directly using grouped convolution feature, but could not find an implemented way of using different sized kernels for the parallel groups\n",
        "    '''\n",
        "    # self.grouped_conv = nn.Conv2d(in_channels=int(cell_in_channels/2), \n",
        "    #                               out_channels=int(cell_out_channels/2),\n",
        "    #                               kernel_size=1, stride=1,groups=2)\n",
        "\n",
        "\n",
        "    ## Used to split the input tensor in half in order to run parallel convolution\n",
        "    self.split_size = int(cell_in_channels/2)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    (path1,path2) = torch.split(x,split_size_or_sections=[self.split_size,self.split_size],dim=1)\n",
        "    path1 = self.cnn1(path1)\n",
        "\n",
        "    path2 = self.cnn3(path2)\n",
        "\n",
        "    x = torch.cat([path1,path2],1)\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.activation_function(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1J1kCbckXmf",
        "colab_type": "text"
      },
      "source": [
        "## Stage Module\n",
        "*Six cells are\n",
        "used to establish the direct connection between different\n",
        "layers, making full use of the feature maps of each layer*\n",
        "\n",
        "![stage](https://i.imgur.com/szNZQ9Cm.jpg)\n",
        "\n",
        "The outputs from each of the cells, as well as the 1 x 1 convolution are accumulated into the input for the next cell\n",
        "\n",
        "The two 1 x 1 convolutions are used to reduce the number of feature maps when connecting between the two stages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6zKrgG20Xbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Stage(nn.Module):\n",
        "  def __init__(self,cell_connections_in, cell_connections_out,stage_in,stage_out):\n",
        "    super(Stage,self).__init__()\n",
        "\n",
        "    self.activation_function = nn.ReLU()\n",
        "    self.batch_norm = nn.BatchNorm2d(k)\n",
        "\n",
        "    self.cnn1 = nn.Conv2d(in_channels=stage_in,\n",
        "                          out_channels=k,kernel_size=1,\n",
        "                          stride=1)\n",
        "    \n",
        "    self.cnn2 = nn.Conv2d(in_channels=7*k,\n",
        "                          out_channels=stage_out,kernel_size=1,\n",
        "                          stride=1)\n",
        "    \n",
        "    ## Densely connected six cell blocks\n",
        "    self.cells = nn.ModuleList([\n",
        "                                Cell(cell_connections_in[i],\n",
        "                                     cell_connections_out[i]) for i in range(6)\n",
        "                                     ])      \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    cell_results = []\n",
        "    x = self.cnn1(x)\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.activation_function(x)\n",
        "\n",
        "    cell_results.append(x)\n",
        "    for i in range(6):\n",
        "      x = torch.cat(cell_results,1)\n",
        "      x = self.cells[i](x)\n",
        "      cell_results.append(x);\n",
        "      \n",
        "    x = torch.cat(cell_results,1)\n",
        "\n",
        "    x = self.cnn2(x)\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.activation_function(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VduCbXykyqJ",
        "colab_type": "text"
      },
      "source": [
        "## Teacher Network\n",
        "![teacher](https://i.imgur.com/bTH8KSCm.jpg)\n",
        "\n",
        "Finally, we define the teacher network which consists of 4 stage modules connected in a dense fashion, with each stage producing a 'k' feature maps where 'k' is the growth rate of the network.\n",
        "\n",
        "Stage 0 takes the input tensor which has 3 x H X W tensor and outputs a k x H x W tensor. The remaining Stages take 'k' feature maps as input and output 'k' feature maps\n",
        "\n",
        "Finally, the Stage 3 output is pooled using a 3 x 3 max pooling with stride of 2\n",
        "and finally a fully connected linear layer which produces the probability vector for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4H42GR4Cl31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TeacherNetwork(nn.Module):\n",
        "  def __init__(self,cell_onnections_in,cell_connections_out,stage_connections_in,stage_connections_out):\n",
        "    super(TeacherNetwork, self).__init__()\n",
        "\n",
        "    self.stages = nn.ModuleList([Stage(cell_onnections_in,cell_connections_out,stage_connections_in[i],stage_connections_out[i]) for i in range(4)])\n",
        "    self.max_pool = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "    self.activation_function = nn.ReLU()\n",
        "    self.linear = nn.Linear(in_features=131072,out_features=43)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    stage_results = []\n",
        "    for i in range(4):\n",
        "      if i != 0:\n",
        "        x = torch.cat(stage_results,1)\n",
        "        x = self.stages[i](x)\n",
        "        stage_results.append(x);\n",
        "\n",
        "      else:\n",
        "        x = self.stages[0](x)\n",
        "        stage_results.append(x)\n",
        "    \n",
        "    x = torch.cat(stage_results,1)\n",
        "    x = self.max_pool(x)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.linear(x)\n",
        "    return x;"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL19QLjS92iX",
        "colab_type": "text"
      },
      "source": [
        "# Validation Function\n",
        "\n",
        "Validates the model against the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbDPqodr4F4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model,data):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for i,(images,labels) in enumerate(data):\n",
        "      images = var(images)\n",
        "      x = model(images)\n",
        "      value,pred = torch.max(x,1)\n",
        "      pred = pred.data.cpu()\n",
        "      total += x.size(0)\n",
        "      correct += torch.sum(pred == labels)\n",
        "    return correct*100./total"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkgxABI4Lx6",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Teacher Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CquiAVXIJjXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = PARAMETERS['batch_size']\n",
        "'''\n",
        "The cells use feature maps from every preceding output in the subsequent cells,\n",
        "increasing the number of feature maps for every next cell by k * 2^(i-1) where \n",
        "i is the cell number. Therefore, the first cell inputs 'k' feature maps and \n",
        "outputs k feature maps, and the last cell(6th) inputs 2^5 * k feature maps and \n",
        "outputs the same number\n",
        "'''\n",
        "cell_connections_in = [k,2*k,3*k,4*k,5*k,6*k]\n",
        "cell_connections_out = [k] * 6\n",
        "\n",
        "'''\n",
        "The stages also use feature maps from every preceding output in the subsequent \n",
        "cells, increasing the number of feature maps for every next stage linearly. \n",
        "This is due to the fact that the 1 x 1 convolution at the end of every stage \n",
        "reduces the output feature maps to size 'k'\n",
        "'''\n",
        "\n",
        "stage_connections_in = [3,k,2*k,3*k]\n",
        "stage_connections_out = [k] * 4"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj6CTooWAKVc",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TildqC4pbvjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_teacher_model():\n",
        "\n",
        "  # Scale learning rate to world size\n",
        "  lr = PARAMETERS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  device = xm.xla_device()  \n",
        "\n",
        "  WRAPPED_MODEL = xmp.MpModelWrapper(TeacherNetwork(cell_connections_in,cell_connections_out,stage_connections_in,stage_connections_out))\n",
        "\n",
        "  if(PARAMETERS['load_from_saved']):\n",
        "    teacher = TeacherNetwork(cell_connections_in,cell_connections_out,stage_connections_in,stage_connections_out)\n",
        "    teacher, PARAMETERS['start_epoch'], loss = loadModel(teacher, PARAMETERS['saved_models'])\n",
        "    PARAMETERS['start_epoch'] += 2\n",
        "    PARAMETERS['load_from_saved'] = False\n",
        "    print(\"Loaded model loss\", loss)\n",
        "    WRAPPED_MODEL = xmp.MpModelWrapper(teacher)\n",
        "\n",
        "  # Only instantiate model weights once in memory.\n",
        "  teacher = WRAPPED_MODEL.to(device)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(teacher.parameters(), lr, weight_decay = PARAMETERS['weight_decay'])\n",
        "\n",
        "  def training_loop(data):\n",
        "    epoch_loss = 0.0\n",
        "    running_loss = 0.0\n",
        "    tracker = xm.RateTracker()\n",
        "    teacher.train()\n",
        "\n",
        "    for i, (images, labels) in enumerate(data):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      prediction = teacher(images)\n",
        "      loss = loss_function(prediction, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      xm.optimizer_step(optimizer)\n",
        "\n",
        "      epoch_loss += prediction.shape[0] * loss.item()\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      tracker.add(PARAMETERS['batch_size'])\n",
        "      if(i % 100 == 0):\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "              xm.get_ordinal(), i, loss.item(), tracker.rate(),\n",
        "              tracker.global_rate(), time.asctime()), flush=True)\n",
        "      \n",
        "    return epoch_loss, running_loss\n",
        "    \n",
        "  def testing_loop(data):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    validation_loss = 0\n",
        "    teacher.eval()\n",
        "    images, labels, pred = None, None, None\n",
        "    with torch.no_grad():\n",
        "      for images, labels in data:\n",
        "        output = teacher(images)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += images.size()[0]\n",
        "        # validation_loss += loss_function(correct, labels)\n",
        "        # print(validation_loss)\n",
        "        # pred = pred.cpu()\n",
        "        # total += output.size(0)\n",
        "        # print(\"testing loop 6\")\n",
        "        # correct += torch.sum(pred == labels)\n",
        "        # print(\"testing loop 7\")\n",
        "    \n",
        "    accuracy = correct * 100.0 / total\n",
        "    \n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, images, pred, labels, validation_loss\n",
        "    \n",
        "\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "\n",
        "  for epoch in range(PARAMETERS['start_epoch'], PARAMETERS['epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_set, [device])\n",
        "    epoch_loss, running_loss = training_loop(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    xm.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': teacher.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': (epoch_loss/len(train_set))\n",
        "              }, PARAMETERS['saved_models'])\n",
        "    \n",
        "    para_loader = pl.ParallelLoader(test_set, [device])\n",
        "    accuracy, data, pred, target, validation_loss = testing_loop(para_loader.per_device_loader(device))\n",
        "    print('Epoch: ', epoch + 1, 'Loss: ', (epoch_loss/len(train_set)),'Accuracy: ',accuracy,'%', 'Validation Loss ', validation_loss)\n",
        "\n",
        "    #logging.INFO('Epoch %s || Loss %s', str(epoch+1), str(epoch_loss/len(train_set)))\n",
        "    losses_df.append({'Epoch': epoch + 1, 'Loss': float(epoch_loss/len(train_set)), 'Accuracy': accuracy}, ignore_index=True)\n",
        "    losses_df.to_csv(\"./training_teacher_tpu.log\", encoding='utf-8', index=False)\n",
        "\n",
        "  return accuracy, data, pred, target, teacher"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWYn1w1rcXHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86f80a0f-b8a2-4232-cfb0-0e62265c4030"
      },
      "source": [
        "def start_training(rank, parameters):\n",
        "  global PARAMETERS\n",
        "  PARAMETERS = parameters\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target, trained_teacher = train_teacher_model()\n",
        "  print(\"Final accuracy = \", accuracy)\n",
        "\n",
        "  torch.save(teacher, 'saved_models_final_teacher_tpu.pth')\n",
        "  if rank == 0:\n",
        "    # Retrieve tensors that are on TPU core 0 and plot.\n",
        "    plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "PARAMETERS['load_from_saved'] = True\n",
        "\n",
        "xmp.spawn(start_training, args=(PARAMETERS, ), nprocs = PARAMETERS['num_cores'],\n",
        "          start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "Epoch:  135 Loss:  2.5766902464875763\n",
            "Loaded model loss 2.5766902464875763\n",
            "[xla:7](0) Loss=0.05618 Rate=0.80 GlobalRate=0.80 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:6](0) Loss=0.05618 Rate=0.79 GlobalRate=0.79 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:1](0) Loss=0.05618 Rate=0.81 GlobalRate=0.81 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:4](0) Loss=0.05618 Rate=0.82 GlobalRate=0.82 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:3](0) Loss=0.05618 Rate=0.80 GlobalRate=0.80 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:2](0) Loss=0.05618 Rate=0.79 GlobalRate=0.79 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:5](0) Loss=0.05618 Rate=0.83 GlobalRate=0.83 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:0](0) Loss=0.05618 Rate=0.76 GlobalRate=0.76 Time=Wed Jul  1 12:35:32 2020\n",
            "[xla:1](100) Loss=0.00093 Rate=8.47 GlobalRate=11.75 Time=Wed Jul  1 12:51:15 2020\n",
            "[xla:4](100) Loss=0.00093 Rate=8.47 GlobalRate=11.77 Time=Wed Jul  1 12:51:15 2020\n",
            "[xla:6](100) Loss=0.00093 Rate=8.46 GlobalRate=11.69 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:0](100) Loss=0.00093 Rate=8.45 GlobalRate=11.62 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:3](100) Loss=0.00093 Rate=8.46 GlobalRate=11.72 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:7](100) Loss=0.00093 Rate=8.46 GlobalRate=11.72 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:5](100) Loss=0.00093 Rate=8.48 GlobalRate=11.79 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:2](100) Loss=0.00093 Rate=8.46 GlobalRate=11.71 Time=Wed Jul  1 12:51:16 2020\n",
            "[xla:7](200) Loss=0.00626 Rate=11.28 GlobalRate=12.39 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:5](200) Loss=0.00626 Rate=11.28 GlobalRate=12.43 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:1](200) Loss=0.00626 Rate=11.27 GlobalRate=12.41 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:4](200) Loss=0.00626 Rate=11.28 GlobalRate=12.41 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:0](200) Loss=0.00626 Rate=11.27 GlobalRate=12.33 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:2](200) Loss=0.00626 Rate=11.27 GlobalRate=12.38 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:6](200) Loss=0.00626 Rate=11.27 GlobalRate=12.37 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:3](200) Loss=0.00626 Rate=11.27 GlobalRate=12.39 Time=Wed Jul  1 13:07:29 2020\n",
            "[xla:3](300) Loss=0.00977 Rate=10.72 GlobalRate=11.63 Time=Wed Jul  1 13:28:06 2020\n",
            "[xla:2](300) Loss=0.00977 Rate=10.72 GlobalRate=11.62 Time=Wed Jul  1 13:28:06 2020\n",
            "[xla:5](300) Loss=0.00977 Rate=10.72 GlobalRate=11.65 Time=Wed Jul  1 13:28:06 2020\n",
            "[xla:0](300) Loss=0.00977 Rate=10.71 GlobalRate=11.59 Time=Wed Jul  1 13:28:06 2020\n",
            "[xla:4](300) Loss=0.00977 Rate=10.72 GlobalRate=11.64 Time=Wed Jul  1 13:28:07 2020\n",
            "[xla:6](300) Loss=0.00977 Rate=10.72 GlobalRate=11.62 Time=Wed Jul  1 13:28:07 2020\n",
            "[xla:1](300) Loss=0.00977 Rate=10.72 GlobalRate=11.63 Time=Wed Jul  1 13:28:07 2020\n",
            "[xla:7](300) Loss=0.00977 Rate=10.72 GlobalRate=11.63 Time=Wed Jul  1 13:28:07 2020\n",
            "Finished training epoch 137\n",
            "[xla:0] Accuracy=90.54%\n",
            "[xla:3] Accuracy=90.54%\n",
            "[xla:4] Accuracy=90.54%\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "[xla:1] Accuracy=90.54%\n",
            "[xla:6] Accuracy=90.54%\n",
            "[xla:2] Accuracy=90.54%\n",
            "[xla:5] Accuracy=90.54%\n",
            "[xla:7] Accuracy=90.54%\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "Epoch:  138 Loss:  3.9448950274177017 Accuracy:  90.53840063341251 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.00328 Rate=23.19 GlobalRate=23.19 Time=Wed Jul  1 13:52:19 2020\n",
            "[xla:6](0) Loss=0.00328 Rate=22.12 GlobalRate=22.12 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:3](0) Loss=0.00328 Rate=21.46 GlobalRate=21.46 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:7](0) Loss=0.00328 Rate=21.29 GlobalRate=21.29 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:2](0) Loss=0.00328 Rate=21.10 GlobalRate=21.10 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:1](0) Loss=0.00328 Rate=20.87 GlobalRate=20.87 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:5](0) Loss=0.00328 Rate=20.65 GlobalRate=20.65 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:0](0) Loss=0.00328 Rate=20.16 GlobalRate=20.16 Time=Wed Jul  1 13:52:20 2020\n",
            "[xla:0](100) Loss=0.01808 Rate=60.01 GlobalRate=83.85 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:4](100) Loss=0.01808 Rate=60.91 GlobalRate=83.80 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:5](100) Loss=0.01808 Rate=60.19 GlobalRate=83.90 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:6](100) Loss=0.01808 Rate=60.64 GlobalRate=83.91 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:3](100) Loss=0.01808 Rate=60.37 GlobalRate=83.81 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:7](100) Loss=0.01808 Rate=60.33 GlobalRate=83.82 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:1](100) Loss=0.01808 Rate=60.23 GlobalRate=83.85 Time=Wed Jul  1 13:54:48 2020\n",
            "[xla:2](100) Loss=0.01808 Rate=60.07 GlobalRate=83.50 Time=Wed Jul  1 13:54:49 2020\n",
            "[xla:1](200) Loss=0.03991 Rate=73.98 GlobalRate=83.50 Time=Wed Jul  1 13:57:22 2020\n",
            "[xla:6](200) Loss=0.03991 Rate=74.13 GlobalRate=83.51 Time=Wed Jul  1 13:57:22 2020\n",
            "[xla:4](200) Loss=0.03991 Rate=74.19 GlobalRate=83.42 Time=Wed Jul  1 13:57:22 2020\n",
            "[xla:0](200) Loss=0.03991 Rate=73.76 GlobalRate=83.39 Time=Wed Jul  1 13:57:23 2020\n",
            "[xla:2](200) Loss=0.03991 Rate=74.02 GlobalRate=83.41 Time=Wed Jul  1 13:57:23 2020\n",
            "[xla:5](200) Loss=0.03991 Rate=73.58 GlobalRate=83.20 Time=Wed Jul  1 13:57:23 2020\n",
            "[xla:7](200) Loss=0.03991 Rate=73.60 GlobalRate=83.13 Time=Wed Jul  1 13:57:24 2020\n",
            "[xla:3](200) Loss=0.03991 Rate=73.61 GlobalRate=83.12 Time=Wed Jul  1 13:57:24 2020\n",
            "[xla:5](300) Loss=0.07666 Rate=79.20 GlobalRate=83.12 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:2](300) Loss=0.07666 Rate=79.14 GlobalRate=83.13 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:4](300) Loss=0.07666 Rate=79.12 GlobalRate=83.08 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:6](300) Loss=0.07666 Rate=79.09 GlobalRate=83.14 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:0](300) Loss=0.07666 Rate=79.01 GlobalRate=83.09 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:3](300) Loss=0.07666 Rate=79.29 GlobalRate=83.10 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:1](300) Loss=0.07666 Rate=79.02 GlobalRate=83.12 Time=Wed Jul  1 13:59:58 2020\n",
            "[xla:7](300) Loss=0.07666 Rate=79.27 GlobalRate=83.11 Time=Wed Jul  1 13:59:58 2020\n",
            "Finished training epoch 138\n",
            "[xla:4] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:1] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:2] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:5] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:6] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:7] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:3] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:0] Accuracy=87.36%\n",
            "Epoch:  139 Loss:  3.5237088763649522 Accuracy:  87.35550277117973 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.05237 Rate=23.54 GlobalRate=23.54 Time=Wed Jul  1 14:03:08 2020\n",
            "[xla:2](0) Loss=0.05237 Rate=21.00 GlobalRate=21.00 Time=Wed Jul  1 14:03:08 2020\n",
            "[xla:1](0) Loss=0.05237 Rate=20.68 GlobalRate=20.68 Time=Wed Jul  1 14:03:08 2020\n",
            "[xla:7](0) Loss=0.05237 Rate=20.37 GlobalRate=20.37 Time=Wed Jul  1 14:03:09 2020\n",
            "[xla:6](0) Loss=0.05237 Rate=20.35 GlobalRate=20.35 Time=Wed Jul  1 14:03:09 2020\n",
            "[xla:5](0) Loss=0.05237 Rate=19.88 GlobalRate=19.88 Time=Wed Jul  1 14:03:09 2020\n",
            "[xla:0](0) Loss=0.05237 Rate=20.67 GlobalRate=20.67 Time=Wed Jul  1 14:03:09 2020\n",
            "[xla:3](0) Loss=0.05237 Rate=20.14 GlobalRate=20.14 Time=Wed Jul  1 14:03:09 2020\n",
            "[xla:0](100) Loss=0.00843 Rate=59.09 GlobalRate=82.18 Time=Wed Jul  1 14:05:40 2020\n",
            "[xla:1](100) Loss=0.00843 Rate=58.90 GlobalRate=81.89 Time=Wed Jul  1 14:05:40 2020\n",
            "[xla:2](100) Loss=0.00843 Rate=59.01 GlobalRate=81.91 Time=Wed Jul  1 14:05:40 2020\n",
            "[xla:3](100) Loss=0.00843 Rate=58.89 GlobalRate=82.11 Time=Wed Jul  1 14:05:40 2020\n",
            "[xla:7](100) Loss=0.00843 Rate=58.76 GlobalRate=81.81 Time=Wed Jul  1 14:05:41 2020\n",
            "[xla:4](100) Loss=0.00843 Rate=59.52 GlobalRate=81.45 Time=Wed Jul  1 14:05:41 2020\n",
            "[xla:5](100) Loss=0.00843 Rate=58.48 GlobalRate=81.60 Time=Wed Jul  1 14:05:41 2020\n",
            "[xla:6](100) Loss=0.00843 Rate=58.64 GlobalRate=81.63 Time=Wed Jul  1 14:05:41 2020\n",
            "[xla:3](200) Loss=0.00100 Rate=72.92 GlobalRate=82.19 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:1](200) Loss=0.00100 Rate=72.92 GlobalRate=82.08 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:2](200) Loss=0.00100 Rate=72.96 GlobalRate=82.08 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:0](200) Loss=0.00100 Rate=72.96 GlobalRate=82.20 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:5](200) Loss=0.00100 Rate=72.97 GlobalRate=82.11 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:4](200) Loss=0.00100 Rate=73.37 GlobalRate=82.02 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:6](200) Loss=0.00100 Rate=73.01 GlobalRate=82.10 Time=Wed Jul  1 14:08:16 2020\n",
            "[xla:7](200) Loss=0.00100 Rate=72.63 GlobalRate=81.84 Time=Wed Jul  1 14:08:17 2020\n",
            "[xla:4](300) Loss=0.02167 Rate=77.65 GlobalRate=81.51 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:5](300) Loss=0.02167 Rate=77.48 GlobalRate=81.56 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:6](300) Loss=0.02167 Rate=77.53 GlobalRate=81.58 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:3](300) Loss=0.02167 Rate=77.43 GlobalRate=81.60 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:1](300) Loss=0.02167 Rate=77.43 GlobalRate=81.53 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:2](300) Loss=0.02167 Rate=77.45 GlobalRate=81.53 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:0](300) Loss=0.02167 Rate=77.45 GlobalRate=81.60 Time=Wed Jul  1 14:10:55 2020\n",
            "[xla:7](300) Loss=0.02167 Rate=77.67 GlobalRate=81.57 Time=Wed Jul  1 14:10:55 2020\n",
            "Finished training epoch 139\n",
            "[xla:4] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:3] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:7] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:1] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:5] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:6] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:0] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:2] Accuracy=80.02%\n",
            "Epoch:  140 Loss:  3.076457605111346 Accuracy:  80.01583531274743 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.24645 Rate=25.87 GlobalRate=25.87 Time=Wed Jul  1 14:14:12 2020\n",
            "[xla:3](0) Loss=0.24645 Rate=21.96 GlobalRate=21.96 Time=Wed Jul  1 14:14:13 2020\n",
            "[xla:7](0) Loss=0.24645 Rate=20.63 GlobalRate=20.63 Time=Wed Jul  1 14:14:14 2020\n",
            "[xla:1](0) Loss=0.24645 Rate=20.21 GlobalRate=20.21 Time=Wed Jul  1 14:14:14 2020\n",
            "[xla:5](0) Loss=0.24645 Rate=19.66 GlobalRate=19.66 Time=Wed Jul  1 14:14:14 2020\n",
            "[xla:2](0) Loss=0.24645 Rate=20.08 GlobalRate=20.08 Time=Wed Jul  1 14:14:14 2020\n",
            "[xla:0](0) Loss=0.24645 Rate=20.00 GlobalRate=20.00 Time=Wed Jul  1 14:14:14 2020\n",
            "[xla:6](0) Loss=0.24645 Rate=19.10 GlobalRate=19.10 Time=Wed Jul  1 14:14:15 2020\n",
            "[xla:4](100) Loss=0.02119 Rate=59.69 GlobalRate=80.51 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:5](100) Loss=0.02119 Rate=57.89 GlobalRate=80.78 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:2](100) Loss=0.02119 Rate=58.10 GlobalRate=80.93 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:3](100) Loss=0.02119 Rate=58.43 GlobalRate=80.54 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:0](100) Loss=0.02119 Rate=58.03 GlobalRate=80.85 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:7](100) Loss=0.02119 Rate=57.89 GlobalRate=80.34 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:6](100) Loss=0.02119 Rate=57.55 GlobalRate=80.51 Time=Wed Jul  1 14:16:48 2020\n",
            "[xla:1](100) Loss=0.02119 Rate=57.77 GlobalRate=80.35 Time=Wed Jul  1 14:16:49 2020\n",
            "[xla:4](200) Loss=0.00220 Rate=72.58 GlobalRate=80.84 Time=Wed Jul  1 14:19:25 2020\n",
            "[xla:5](200) Loss=0.00220 Rate=71.87 GlobalRate=80.98 Time=Wed Jul  1 14:19:25 2020\n",
            "[xla:2](200) Loss=0.00220 Rate=71.97 GlobalRate=81.07 Time=Wed Jul  1 14:19:25 2020\n",
            "[xla:6](200) Loss=0.00220 Rate=71.93 GlobalRate=81.01 Time=Wed Jul  1 14:19:25 2020\n",
            "[xla:3](200) Loss=0.00220 Rate=72.10 GlobalRate=80.87 Time=Wed Jul  1 14:19:25 2020\n",
            "[xla:0](200) Loss=0.00220 Rate=71.95 GlobalRate=81.04 Time=Wed Jul  1 14:19:26 2020\n",
            "[xla:7](200) Loss=0.00220 Rate=71.92 GlobalRate=80.81 Time=Wed Jul  1 14:19:26 2020\n",
            "[xla:1](200) Loss=0.00220 Rate=71.83 GlobalRate=80.77 Time=Wed Jul  1 14:19:26 2020\n",
            "[xla:7](300) Loss=0.00202 Rate=76.61 GlobalRate=80.45 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:6](300) Loss=0.00202 Rate=76.48 GlobalRate=80.50 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:3](300) Loss=0.00202 Rate=76.53 GlobalRate=80.41 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:2](300) Loss=0.00202 Rate=76.47 GlobalRate=80.53 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:0](300) Loss=0.00202 Rate=76.49 GlobalRate=80.53 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:5](300) Loss=0.00202 Rate=76.42 GlobalRate=80.47 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:4](300) Loss=0.00202 Rate=76.70 GlobalRate=80.37 Time=Wed Jul  1 14:22:06 2020\n",
            "[xla:1](300) Loss=0.00202 Rate=76.62 GlobalRate=80.45 Time=Wed Jul  1 14:22:07 2020\n",
            "Finished training epoch 140\n",
            "[xla:3] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:7] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:4] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:5] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:6] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:1] Accuracy=86.72%\n",
            "[xla:2] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:0] Accuracy=86.72%\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "Epoch:  141 Loss:  3.357674705195286 Accuracy:  86.72209026128266 % Validation Loss  0\n",
            "[xla:3](0) Loss=0.00219 Rate=23.58 GlobalRate=23.58 Time=Wed Jul  1 14:25:16 2020\n",
            "[xla:7](0) Loss=0.00219 Rate=21.09 GlobalRate=21.09 Time=Wed Jul  1 14:25:17 2020\n",
            "[xla:4](0) Loss=0.00219 Rate=21.20 GlobalRate=21.20 Time=Wed Jul  1 14:25:17 2020\n",
            "[xla:6](0) Loss=0.00219 Rate=19.86 GlobalRate=19.86 Time=Wed Jul  1 14:25:18 2020\n",
            "[xla:1](0) Loss=0.00219 Rate=19.53 GlobalRate=19.53 Time=Wed Jul  1 14:25:18 2020\n",
            "[xla:2](0) Loss=0.00219 Rate=20.51 GlobalRate=20.51 Time=Wed Jul  1 14:25:18 2020\n",
            "[xla:5](0) Loss=0.00219 Rate=18.61 GlobalRate=18.61 Time=Wed Jul  1 14:25:18 2020\n",
            "[xla:0](0) Loss=0.00219 Rate=20.38 GlobalRate=20.38 Time=Wed Jul  1 14:25:18 2020\n",
            "[xla:7](100) Loss=0.01204 Rate=56.91 GlobalRate=78.59 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:4](100) Loss=0.01204 Rate=57.00 GlobalRate=78.68 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:3](100) Loss=0.01204 Rate=57.67 GlobalRate=78.53 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:0](100) Loss=0.01204 Rate=56.99 GlobalRate=79.05 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:2](100) Loss=0.01204 Rate=57.00 GlobalRate=79.00 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:6](100) Loss=0.01204 Rate=56.65 GlobalRate=78.76 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:1](100) Loss=0.01204 Rate=56.60 GlobalRate=78.84 Time=Wed Jul  1 14:27:55 2020\n",
            "[xla:5](100) Loss=0.01204 Rate=56.14 GlobalRate=78.55 Time=Wed Jul  1 14:27:56 2020\n",
            "[xla:3](200) Loss=0.01076 Rate=71.46 GlobalRate=79.57 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:2](200) Loss=0.01076 Rate=71.21 GlobalRate=79.83 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:1](200) Loss=0.01076 Rate=71.05 GlobalRate=79.75 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:4](200) Loss=0.01076 Rate=71.17 GlobalRate=79.63 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:0](200) Loss=0.01076 Rate=71.17 GlobalRate=79.83 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:7](200) Loss=0.01076 Rate=71.12 GlobalRate=79.58 Time=Wed Jul  1 14:30:34 2020\n",
            "[xla:5](200) Loss=0.01076 Rate=70.74 GlobalRate=79.50 Time=Wed Jul  1 14:30:35 2020\n",
            "[xla:6](200) Loss=0.01076 Rate=70.72 GlobalRate=79.43 Time=Wed Jul  1 14:30:35 2020\n",
            "[xla:6](300) Loss=0.00149 Rate=76.66 GlobalRate=79.82 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:2](300) Loss=0.00149 Rate=76.50 GlobalRate=79.90 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:0](300) Loss=0.00149 Rate=76.49 GlobalRate=79.90 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:7](300) Loss=0.00149 Rate=76.44 GlobalRate=79.71 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:3](300) Loss=0.00149 Rate=76.54 GlobalRate=79.69 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:4](300) Loss=0.00149 Rate=76.45 GlobalRate=79.74 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:1](300) Loss=0.00149 Rate=76.40 GlobalRate=79.82 Time=Wed Jul  1 14:33:14 2020\n",
            "[xla:5](300) Loss=0.00149 Rate=76.50 GlobalRate=79.77 Time=Wed Jul  1 14:33:14 2020\n",
            "Finished training epoch 141\n",
            "[xla:1] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:2] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:0] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:5] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:4] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:6] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:7] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:3] Accuracy=89.18%\n",
            "Epoch:  142 Loss:  2.757157584191194 Accuracy:  89.1765637371338 % Validation Loss  0\n",
            "[xla:1](0) Loss=0.00067 Rate=26.27 GlobalRate=26.27 Time=Wed Jul  1 14:36:34 2020\n",
            "[xla:2](0) Loss=0.00067 Rate=25.06 GlobalRate=25.06 Time=Wed Jul  1 14:36:34 2020\n",
            "[xla:0](0) Loss=0.00067 Rate=19.28 GlobalRate=19.28 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:5](0) Loss=0.00067 Rate=19.42 GlobalRate=19.42 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:4](0) Loss=0.00067 Rate=19.30 GlobalRate=19.30 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:7](0) Loss=0.00067 Rate=19.85 GlobalRate=19.85 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:6](0) Loss=0.00067 Rate=19.49 GlobalRate=19.49 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:3](0) Loss=0.00067 Rate=19.45 GlobalRate=19.45 Time=Wed Jul  1 14:36:36 2020\n",
            "[xla:1](100) Loss=0.00061 Rate=58.40 GlobalRate=78.24 Time=Wed Jul  1 14:39:14 2020\n",
            "[xla:7](100) Loss=0.00061 Rate=56.60 GlobalRate=78.70 Time=Wed Jul  1 14:39:14 2020\n",
            "[xla:6](100) Loss=0.00061 Rate=56.49 GlobalRate=78.69 Time=Wed Jul  1 14:39:14 2020\n",
            "[xla:5](100) Loss=0.00061 Rate=56.35 GlobalRate=78.51 Time=Wed Jul  1 14:39:14 2020\n",
            "[xla:2](100) Loss=0.00061 Rate=57.73 GlobalRate=77.84 Time=Wed Jul  1 14:39:15 2020\n",
            "[xla:0](100) Loss=0.00061 Rate=55.99 GlobalRate=78.01 Time=Wed Jul  1 14:39:15 2020\n",
            "[xla:3](100) Loss=0.00061 Rate=56.16 GlobalRate=78.19 Time=Wed Jul  1 14:39:15 2020\n",
            "[xla:4](100) Loss=0.00061 Rate=55.98 GlobalRate=77.99 Time=Wed Jul  1 14:39:15 2020\n",
            "[xla:6](200) Loss=0.01993 Rate=70.90 GlobalRate=79.59 Time=Wed Jul  1 14:41:53 2020\n",
            "[xla:7](200) Loss=0.01993 Rate=70.93 GlobalRate=79.57 Time=Wed Jul  1 14:41:53 2020\n",
            "[xla:1](200) Loss=0.01993 Rate=71.63 GlobalRate=79.32 Time=Wed Jul  1 14:41:53 2020\n",
            "[xla:3](200) Loss=0.01993 Rate=71.08 GlobalRate=79.58 Time=Wed Jul  1 14:41:53 2020\n",
            "[xla:2](200) Loss=0.01993 Rate=71.52 GlobalRate=79.24 Time=Wed Jul  1 14:41:53 2020\n",
            "[xla:4](200) Loss=0.01993 Rate=70.91 GlobalRate=79.39 Time=Wed Jul  1 14:41:54 2020\n",
            "[xla:0](200) Loss=0.01993 Rate=70.77 GlobalRate=79.29 Time=Wed Jul  1 14:41:54 2020\n",
            "[xla:5](200) Loss=0.01993 Rate=70.56 GlobalRate=79.26 Time=Wed Jul  1 14:41:54 2020\n",
            "[xla:6](300) Loss=0.00818 Rate=75.88 GlobalRate=79.46 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:4](300) Loss=0.00818 Rate=76.05 GlobalRate=79.42 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:0](300) Loss=0.00818 Rate=76.06 GlobalRate=79.39 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:1](300) Loss=0.00818 Rate=76.19 GlobalRate=79.29 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:7](300) Loss=0.00818 Rate=75.89 GlobalRate=79.45 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:2](300) Loss=0.00818 Rate=76.23 GlobalRate=79.28 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:5](300) Loss=0.00818 Rate=76.02 GlobalRate=79.39 Time=Wed Jul  1 14:44:35 2020\n",
            "[xla:3](300) Loss=0.00818 Rate=75.96 GlobalRate=79.46 Time=Wed Jul  1 14:44:35 2020\n",
            "Finished training epoch 142\n",
            "[xla:1] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:4] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:3] Accuracy=84.55%\n",
            "[xla:0] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:6] Accuracy=84.55%\n",
            "[xla:2] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:5] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:7] Accuracy=84.55%\n",
            "Epoch:  143 Loss:  2.7323399785032394 Accuracy:  84.55265241488519 % Validation Loss  0\n",
            "[xla:1](0) Loss=0.09025 Rate=23.14 GlobalRate=23.14 Time=Wed Jul  1 14:47:49 2020\n",
            "[xla:7](0) Loss=0.09025 Rate=21.23 GlobalRate=21.23 Time=Wed Jul  1 14:47:50 2020\n",
            "[xla:0](0) Loss=0.09025 Rate=20.58 GlobalRate=20.58 Time=Wed Jul  1 14:47:50 2020\n",
            "[xla:4](0) Loss=0.09025 Rate=19.93 GlobalRate=19.93 Time=Wed Jul  1 14:47:50 2020\n",
            "[xla:2](0) Loss=0.09025 Rate=20.28 GlobalRate=20.28 Time=Wed Jul  1 14:47:50 2020\n",
            "[xla:5](0) Loss=0.09025 Rate=20.51 GlobalRate=20.51 Time=Wed Jul  1 14:47:50 2020\n",
            "[xla:3](0) Loss=0.09025 Rate=19.12 GlobalRate=19.12 Time=Wed Jul  1 14:47:51 2020\n",
            "[xla:6](0) Loss=0.09025 Rate=19.20 GlobalRate=19.20 Time=Wed Jul  1 14:47:51 2020\n",
            "[xla:5](100) Loss=0.01630 Rate=57.40 GlobalRate=79.63 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:1](100) Loss=0.01630 Rate=58.07 GlobalRate=79.37 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:7](100) Loss=0.01630 Rate=57.60 GlobalRate=79.60 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:2](100) Loss=0.01630 Rate=57.25 GlobalRate=79.51 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:6](100) Loss=0.01630 Rate=56.93 GlobalRate=79.51 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:0](100) Loss=0.01630 Rate=57.34 GlobalRate=79.50 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:3](100) Loss=0.01630 Rate=56.79 GlobalRate=79.33 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:4](100) Loss=0.01630 Rate=56.86 GlobalRate=79.06 Time=Wed Jul  1 14:50:27 2020\n",
            "[xla:1](200) Loss=0.01618 Rate=71.09 GlobalRate=79.57 Time=Wed Jul  1 14:53:07 2020\n",
            "[xla:6](200) Loss=0.01618 Rate=70.63 GlobalRate=79.63 Time=Wed Jul  1 14:53:07 2020\n",
            "[xla:7](200) Loss=0.01618 Rate=70.89 GlobalRate=79.67 Time=Wed Jul  1 14:53:07 2020\n",
            "[xla:5](200) Loss=0.01618 Rate=70.77 GlobalRate=79.65 Time=Wed Jul  1 14:53:07 2020\n",
            "[xla:2](200) Loss=0.01618 Rate=70.57 GlobalRate=79.48 Time=Wed Jul  1 14:53:08 2020\n",
            "[xla:0](200) Loss=0.01618 Rate=70.54 GlobalRate=79.42 Time=Wed Jul  1 14:53:08 2020\n",
            "[xla:3](200) Loss=0.01618 Rate=70.35 GlobalRate=79.36 Time=Wed Jul  1 14:53:08 2020\n",
            "[xla:4](200) Loss=0.01618 Rate=70.47 GlobalRate=79.30 Time=Wed Jul  1 14:53:08 2020\n",
            "[xla:1](300) Loss=0.01877 Rate=76.25 GlobalRate=79.61 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:7](300) Loss=0.01877 Rate=76.18 GlobalRate=79.69 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:0](300) Loss=0.01877 Rate=76.30 GlobalRate=79.66 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:2](300) Loss=0.01877 Rate=76.23 GlobalRate=79.66 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:3](300) Loss=0.01877 Rate=76.28 GlobalRate=79.65 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:6](300) Loss=0.01877 Rate=76.07 GlobalRate=79.66 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:5](300) Loss=0.01877 Rate=76.14 GlobalRate=79.68 Time=Wed Jul  1 14:55:48 2020\n",
            "[xla:4](300) Loss=0.01877 Rate=76.35 GlobalRate=79.62 Time=Wed Jul  1 14:55:48 2020\n",
            "Finished training epoch 143\n",
            "[xla:6] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:1] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:7] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:0] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:4] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:2] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:5] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:3] Accuracy=87.45%\n",
            "Epoch:  144 Loss:  4.022788546448544 Accuracy:  87.45051464766429 % Validation Loss  0\n",
            "[xla:6](0) Loss=0.00049 Rate=22.66 GlobalRate=22.66 Time=Wed Jul  1 14:59:06 2020\n",
            "[xla:1](0) Loss=0.00049 Rate=21.14 GlobalRate=21.14 Time=Wed Jul  1 14:59:06 2020\n",
            "[xla:7](0) Loss=0.00049 Rate=20.14 GlobalRate=20.14 Time=Wed Jul  1 14:59:07 2020\n",
            "[xla:0](0) Loss=0.00049 Rate=20.25 GlobalRate=20.25 Time=Wed Jul  1 14:59:07 2020\n",
            "[xla:2](0) Loss=0.00049 Rate=19.62 GlobalRate=19.62 Time=Wed Jul  1 14:59:07 2020\n",
            "[xla:4](0) Loss=0.00049 Rate=19.15 GlobalRate=19.15 Time=Wed Jul  1 14:59:07 2020\n",
            "[xla:3](0) Loss=0.00049 Rate=19.70 GlobalRate=19.70 Time=Wed Jul  1 14:59:08 2020\n",
            "[xla:5](0) Loss=0.00049 Rate=19.10 GlobalRate=19.10 Time=Wed Jul  1 14:59:08 2020\n",
            "[xla:2](100) Loss=0.03392 Rate=56.25 GlobalRate=78.25 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:1](100) Loss=0.03392 Rate=56.49 GlobalRate=77.91 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:0](100) Loss=0.03392 Rate=56.35 GlobalRate=78.12 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:4](100) Loss=0.03392 Rate=56.07 GlobalRate=78.20 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:7](100) Loss=0.03392 Rate=56.25 GlobalRate=78.02 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:3](100) Loss=0.03392 Rate=56.36 GlobalRate=78.39 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:6](100) Loss=0.03392 Rate=56.96 GlobalRate=77.87 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:5](100) Loss=0.03392 Rate=56.06 GlobalRate=78.20 Time=Wed Jul  1 15:01:46 2020\n",
            "[xla:4](200) Loss=0.00231 Rate=70.26 GlobalRate=78.95 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:3](200) Loss=0.00231 Rate=70.37 GlobalRate=79.04 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:7](200) Loss=0.00231 Rate=70.32 GlobalRate=78.84 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:2](200) Loss=0.00231 Rate=70.31 GlobalRate=78.96 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:5](200) Loss=0.00231 Rate=70.35 GlobalRate=79.03 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:1](200) Loss=0.00231 Rate=70.41 GlobalRate=78.78 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:6](200) Loss=0.00231 Rate=70.46 GlobalRate=78.66 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:0](200) Loss=0.00231 Rate=70.19 GlobalRate=78.76 Time=Wed Jul  1 15:04:27 2020\n",
            "[xla:2](300) Loss=0.00162 Rate=76.16 GlobalRate=79.32 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:0](300) Loss=0.00162 Rate=76.29 GlobalRate=79.28 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:5](300) Loss=0.00162 Rate=76.15 GlobalRate=79.35 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:1](300) Loss=0.00162 Rate=76.17 GlobalRate=79.19 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:3](300) Loss=0.00162 Rate=76.15 GlobalRate=79.36 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:4](300) Loss=0.00162 Rate=76.09 GlobalRate=79.29 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:7](300) Loss=0.00162 Rate=76.12 GlobalRate=79.22 Time=Wed Jul  1 15:07:07 2020\n",
            "[xla:6](300) Loss=0.00162 Rate=76.32 GlobalRate=79.17 Time=Wed Jul  1 15:07:07 2020\n",
            "Finished training epoch 144\n",
            "[xla:1] Accuracy=85.70%\n",
            "[xla:6] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:3] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:5] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:7] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:2] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:4] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:0] Accuracy=85.70%\n",
            "Epoch:  145 Loss:  1.830606598587436 Accuracy:  85.70071258907363 % Validation Loss  0\n",
            "[xla:6](0) Loss=0.00076 Rate=20.53 GlobalRate=20.53 Time=Wed Jul  1 15:10:27 2020\n",
            "[xla:1](0) Loss=0.00076 Rate=20.11 GlobalRate=20.11 Time=Wed Jul  1 15:10:27 2020\n",
            "[xla:3](0) Loss=0.00076 Rate=20.15 GlobalRate=20.15 Time=Wed Jul  1 15:10:27 2020\n",
            "[xla:7](0) Loss=0.00076 Rate=19.22 GlobalRate=19.22 Time=Wed Jul  1 15:10:28 2020\n",
            "[xla:2](0) Loss=0.00076 Rate=19.10 GlobalRate=19.10 Time=Wed Jul  1 15:10:28 2020\n",
            "[xla:5](0) Loss=0.00076 Rate=18.56 GlobalRate=18.56 Time=Wed Jul  1 15:10:28 2020\n",
            "[xla:4](0) Loss=0.00076 Rate=19.79 GlobalRate=19.79 Time=Wed Jul  1 15:10:28 2020\n",
            "[xla:0](0) Loss=0.00076 Rate=19.71 GlobalRate=19.71 Time=Wed Jul  1 15:10:28 2020\n",
            "[xla:3](100) Loss=0.00457 Rate=56.76 GlobalRate=78.80 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:1](100) Loss=0.00457 Rate=56.69 GlobalRate=78.72 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:6](100) Loss=0.00457 Rate=56.82 GlobalRate=78.72 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:5](100) Loss=0.00457 Rate=56.32 GlobalRate=78.85 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:2](100) Loss=0.00457 Rate=56.53 GlobalRate=78.94 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:4](100) Loss=0.00457 Rate=56.86 GlobalRate=79.12 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:7](100) Loss=0.00457 Rate=56.55 GlobalRate=78.91 Time=Wed Jul  1 15:13:05 2020\n",
            "[xla:0](100) Loss=0.00457 Rate=56.70 GlobalRate=78.92 Time=Wed Jul  1 15:13:06 2020\n",
            "[xla:3](200) Loss=0.00536 Rate=70.44 GlobalRate=79.17 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:5](200) Loss=0.00536 Rate=70.26 GlobalRate=79.20 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:2](200) Loss=0.00536 Rate=70.35 GlobalRate=79.25 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:4](200) Loss=0.00536 Rate=70.48 GlobalRate=79.34 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:7](200) Loss=0.00536 Rate=70.34 GlobalRate=79.22 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:6](200) Loss=0.00536 Rate=70.45 GlobalRate=79.12 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:1](200) Loss=0.00536 Rate=70.39 GlobalRate=79.11 Time=Wed Jul  1 15:15:46 2020\n",
            "[xla:0](200) Loss=0.00536 Rate=70.32 GlobalRate=79.16 Time=Wed Jul  1 15:15:47 2020\n",
            "[xla:7](300) Loss=0.22555 Rate=75.65 GlobalRate=79.21 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:6](300) Loss=0.22555 Rate=75.69 GlobalRate=79.14 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:5](300) Loss=0.22555 Rate=75.60 GlobalRate=79.19 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:0](300) Loss=0.22555 Rate=75.86 GlobalRate=79.29 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:4](300) Loss=0.22555 Rate=75.68 GlobalRate=79.27 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:3](300) Loss=0.22555 Rate=75.66 GlobalRate=79.16 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:2](300) Loss=0.22555 Rate=75.62 GlobalRate=79.21 Time=Wed Jul  1 15:18:28 2020\n",
            "[xla:1](300) Loss=0.22555 Rate=75.65 GlobalRate=79.13 Time=Wed Jul  1 15:18:28 2020\n",
            "Finished training epoch 145\n",
            "[xla:5] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:6] Accuracy=87.97%\n",
            "[xla:3] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:2] Accuracy=87.97%\n",
            "[xla:1] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:7] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:0] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:4] Accuracy=87.97%\n",
            "Epoch:  146 Loss:  3.1444717951677132 Accuracy:  87.97307996832937 % Validation Loss  0\n",
            "[xla:5](0) Loss=0.15296 Rate=20.16 GlobalRate=20.16 Time=Wed Jul  1 15:21:49 2020\n",
            "[xla:3](0) Loss=0.15296 Rate=18.31 GlobalRate=18.31 Time=Wed Jul  1 15:21:50 2020\n",
            "[xla:1](0) Loss=0.15296 Rate=18.90 GlobalRate=18.90 Time=Wed Jul  1 15:21:50 2020\n",
            "[xla:6](0) Loss=0.15296 Rate=18.09 GlobalRate=18.09 Time=Wed Jul  1 15:21:50 2020\n",
            "[xla:2](0) Loss=0.15296 Rate=18.51 GlobalRate=18.51 Time=Wed Jul  1 15:21:50 2020\n",
            "[xla:7](0) Loss=0.15296 Rate=18.73 GlobalRate=18.73 Time=Wed Jul  1 15:21:50 2020\n",
            "[xla:4](0) Loss=0.15296 Rate=18.46 GlobalRate=18.46 Time=Wed Jul  1 15:21:51 2020\n",
            "[xla:0](0) Loss=0.15296 Rate=18.10 GlobalRate=18.10 Time=Wed Jul  1 15:21:51 2020\n",
            "[xla:3](100) Loss=0.05022 Rate=55.44 GlobalRate=77.59 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:1](100) Loss=0.05022 Rate=55.66 GlobalRate=77.68 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:6](100) Loss=0.05022 Rate=55.35 GlobalRate=77.55 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:2](100) Loss=0.05022 Rate=55.49 GlobalRate=77.59 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:5](100) Loss=0.05022 Rate=55.87 GlobalRate=77.42 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:7](100) Loss=0.05022 Rate=55.58 GlobalRate=77.64 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:4](100) Loss=0.05022 Rate=55.55 GlobalRate=77.70 Time=Wed Jul  1 15:24:30 2020\n",
            "[xla:0](100) Loss=0.05022 Rate=55.23 GlobalRate=77.36 Time=Wed Jul  1 15:24:31 2020\n",
            "[xla:4](200) Loss=0.02411 Rate=69.62 GlobalRate=78.34 Time=Wed Jul  1 15:27:12 2020\n",
            "[xla:7](200) Loss=0.02411 Rate=69.60 GlobalRate=78.28 Time=Wed Jul  1 15:27:12 2020\n",
            "[xla:3](200) Loss=0.02411 Rate=69.45 GlobalRate=78.19 Time=Wed Jul  1 15:27:12 2020\n",
            "[xla:6](200) Loss=0.02411 Rate=69.43 GlobalRate=78.18 Time=Wed Jul  1 15:27:12 2020\n",
            "[xla:5](200) Loss=0.02411 Rate=69.68 GlobalRate=78.14 Time=Wed Jul  1 15:27:12 2020\n",
            "[xla:1](200) Loss=0.02411 Rate=69.41 GlobalRate=78.12 Time=Wed Jul  1 15:27:13 2020\n",
            "[xla:0](200) Loss=0.02411 Rate=69.36 GlobalRate=78.06 Time=Wed Jul  1 15:27:13 2020\n",
            "[xla:2](200) Loss=0.02411 Rate=69.16 GlobalRate=77.92 Time=Wed Jul  1 15:27:14 2020\n",
            "[xla:0](300) Loss=0.00145 Rate=74.63 GlobalRate=78.09 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:2](300) Loss=0.00145 Rate=74.60 GlobalRate=78.02 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:6](300) Loss=0.00145 Rate=74.34 GlobalRate=77.99 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:3](300) Loss=0.00145 Rate=74.34 GlobalRate=77.99 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:7](300) Loss=0.00145 Rate=74.38 GlobalRate=78.04 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:1](300) Loss=0.00145 Rate=74.47 GlobalRate=78.03 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:4](300) Loss=0.00145 Rate=74.38 GlobalRate=78.08 Time=Wed Jul  1 15:29:57 2020\n",
            "[xla:5](300) Loss=0.00145 Rate=74.44 GlobalRate=77.97 Time=Wed Jul  1 15:29:57 2020\n",
            "Finished training epoch 146\n",
            "[xla:4] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:5] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:7] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:1] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:3] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:2] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:6] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:0] Accuracy=84.22%\n",
            "Epoch:  147 Loss:  4.624867297235743 Accuracy:  84.22011084718923 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.01508 Rate=23.27 GlobalRate=23.27 Time=Wed Jul  1 15:33:18 2020\n",
            "[xla:5](0) Loss=0.01508 Rate=21.51 GlobalRate=21.51 Time=Wed Jul  1 15:33:19 2020\n",
            "[xla:1](0) Loss=0.01508 Rate=19.87 GlobalRate=19.87 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:3](0) Loss=0.01508 Rate=19.99 GlobalRate=19.99 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:7](0) Loss=0.01508 Rate=19.43 GlobalRate=19.43 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:2](0) Loss=0.01508 Rate=20.03 GlobalRate=20.03 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:6](0) Loss=0.01508 Rate=20.09 GlobalRate=20.09 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:0](0) Loss=0.01508 Rate=19.27 GlobalRate=19.27 Time=Wed Jul  1 15:33:20 2020\n",
            "[xla:1](100) Loss=0.10795 Rate=56.33 GlobalRate=78.27 Time=Wed Jul  1 15:35:58 2020\n",
            "[xla:5](100) Loss=0.10795 Rate=56.76 GlobalRate=78.15 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:6](100) Loss=0.10795 Rate=56.49 GlobalRate=78.41 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:3](100) Loss=0.10795 Rate=56.38 GlobalRate=78.29 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:4](100) Loss=0.10795 Rate=57.26 GlobalRate=78.04 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:7](100) Loss=0.10795 Rate=56.15 GlobalRate=78.20 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:0](100) Loss=0.10795 Rate=56.26 GlobalRate=78.44 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:2](100) Loss=0.10795 Rate=56.17 GlobalRate=77.94 Time=Wed Jul  1 15:35:59 2020\n",
            "[xla:5](200) Loss=0.01718 Rate=70.06 GlobalRate=78.53 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:1](200) Loss=0.01718 Rate=69.88 GlobalRate=78.59 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:6](200) Loss=0.01718 Rate=69.95 GlobalRate=78.67 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:2](200) Loss=0.01718 Rate=70.06 GlobalRate=78.62 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:0](200) Loss=0.01718 Rate=69.89 GlobalRate=78.71 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:3](200) Loss=0.01718 Rate=69.90 GlobalRate=78.60 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:4](200) Loss=0.01718 Rate=70.17 GlobalRate=78.41 Time=Wed Jul  1 15:38:41 2020\n",
            "[xla:7](200) Loss=0.01718 Rate=69.48 GlobalRate=78.28 Time=Wed Jul  1 15:38:42 2020\n",
            "[xla:5](300) Loss=0.02896 Rate=74.67 GlobalRate=78.27 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:6](300) Loss=0.02896 Rate=74.63 GlobalRate=78.36 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:0](300) Loss=0.02896 Rate=74.61 GlobalRate=78.39 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:3](300) Loss=0.02896 Rate=74.61 GlobalRate=78.32 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:2](300) Loss=0.02896 Rate=74.66 GlobalRate=78.32 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:4](300) Loss=0.02896 Rate=74.79 GlobalRate=78.23 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:1](300) Loss=0.02896 Rate=74.58 GlobalRate=78.29 Time=Wed Jul  1 15:41:25 2020\n",
            "[xla:7](300) Loss=0.02896 Rate=74.77 GlobalRate=78.29 Time=Wed Jul  1 15:41:25 2020\n",
            "Finished training epoch 147\n",
            "[xla:2] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:7] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:6] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:5] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:4] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:1] Accuracy=87.87%\n",
            "[xla:3] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:0] Accuracy=87.87%\n",
            "Epoch:  148 Loss:  2.954696301234346 Accuracy:  87.8701504354711 % Validation Loss  0\n",
            "[xla:2](0) Loss=0.02295 Rate=22.89 GlobalRate=22.89 Time=Wed Jul  1 15:44:47 2020\n",
            "[xla:7](0) Loss=0.02295 Rate=20.97 GlobalRate=20.97 Time=Wed Jul  1 15:44:48 2020\n",
            "[xla:5](0) Loss=0.02295 Rate=20.26 GlobalRate=20.26 Time=Wed Jul  1 15:44:48 2020\n",
            "[xla:6](0) Loss=0.02295 Rate=19.57 GlobalRate=19.57 Time=Wed Jul  1 15:44:48 2020\n",
            "[xla:4](0) Loss=0.02295 Rate=19.69 GlobalRate=19.69 Time=Wed Jul  1 15:44:49 2020\n",
            "[xla:1](0) Loss=0.02295 Rate=19.65 GlobalRate=19.65 Time=Wed Jul  1 15:44:49 2020\n",
            "[xla:3](0) Loss=0.02295 Rate=19.34 GlobalRate=19.34 Time=Wed Jul  1 15:44:49 2020\n",
            "[xla:0](0) Loss=0.02295 Rate=19.56 GlobalRate=19.56 Time=Wed Jul  1 15:44:49 2020\n",
            "[xla:3](100) Loss=0.00426 Rate=56.50 GlobalRate=78.77 Time=Wed Jul  1 15:47:26 2020\n",
            "[xla:2](100) Loss=0.00426 Rate=57.33 GlobalRate=78.35 Time=Wed Jul  1 15:47:27 2020\n",
            "[xla:4](100) Loss=0.00426 Rate=56.48 GlobalRate=78.58 Time=Wed Jul  1 15:47:27 2020\n",
            "[xla:5](100) Loss=0.00426 Rate=56.48 GlobalRate=78.31 Time=Wed Jul  1 15:47:27 2020\n",
            "[xla:6](100) Loss=0.00426 Rate=56.24 GlobalRate=78.27 Time=Wed Jul  1 15:47:27 2020\n",
            "[xla:0](100) Loss=0.00426 Rate=56.21 GlobalRate=78.23 Time=Wed Jul  1 15:47:28 2020\n",
            "[xla:7](100) Loss=0.00426 Rate=56.38 GlobalRate=77.82 Time=Wed Jul  1 15:47:28 2020\n",
            "[xla:1](100) Loss=0.00426 Rate=55.99 GlobalRate=77.85 Time=Wed Jul  1 15:47:28 2020\n",
            "[xla:6](200) Loss=0.05571 Rate=70.11 GlobalRate=78.81 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:5](200) Loss=0.05571 Rate=70.21 GlobalRate=78.83 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:3](200) Loss=0.05571 Rate=70.00 GlobalRate=78.88 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:2](200) Loss=0.05571 Rate=70.43 GlobalRate=78.75 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:4](200) Loss=0.05571 Rate=70.09 GlobalRate=78.87 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:0](200) Loss=0.05571 Rate=70.25 GlobalRate=78.91 Time=Wed Jul  1 15:50:08 2020\n",
            "[xla:1](200) Loss=0.05571 Rate=70.14 GlobalRate=78.69 Time=Wed Jul  1 15:50:09 2020\n",
            "[xla:7](200) Loss=0.05571 Rate=70.11 GlobalRate=78.53 Time=Wed Jul  1 15:50:09 2020\n",
            "[xla:4](300) Loss=0.00059 Rate=75.08 GlobalRate=78.71 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:2](300) Loss=0.00059 Rate=75.21 GlobalRate=78.63 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:3](300) Loss=0.00059 Rate=75.03 GlobalRate=78.72 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:5](300) Loss=0.00059 Rate=75.10 GlobalRate=78.67 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:6](300) Loss=0.00059 Rate=75.06 GlobalRate=78.66 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:0](300) Loss=0.00059 Rate=75.15 GlobalRate=78.74 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:1](300) Loss=0.00059 Rate=75.33 GlobalRate=78.72 Time=Wed Jul  1 15:52:52 2020\n",
            "[xla:7](300) Loss=0.00059 Rate=75.37 GlobalRate=78.65 Time=Wed Jul  1 15:52:52 2020\n",
            "Finished training epoch 148\n",
            "[xla:4] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:1] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:5] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:6] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:2] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:3] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:7] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:0] Accuracy=72.03%\n",
            "Epoch:  149 Loss:  2.0741098603781265 Accuracy:  72.03483768804433 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.00833 Rate=22.18 GlobalRate=22.18 Time=Wed Jul  1 15:56:10 2020\n",
            "[xla:1](0) Loss=0.00833 Rate=20.51 GlobalRate=20.51 Time=Wed Jul  1 15:56:10 2020\n",
            "[xla:5](0) Loss=0.00833 Rate=20.39 GlobalRate=20.39 Time=Wed Jul  1 15:56:10 2020\n",
            "[xla:6](0) Loss=0.00833 Rate=19.67 GlobalRate=19.67 Time=Wed Jul  1 15:56:11 2020\n",
            "[xla:2](0) Loss=0.00833 Rate=19.65 GlobalRate=19.65 Time=Wed Jul  1 15:56:11 2020\n",
            "[xla:3](0) Loss=0.00833 Rate=19.62 GlobalRate=19.62 Time=Wed Jul  1 15:56:11 2020\n",
            "[xla:7](0) Loss=0.00833 Rate=19.47 GlobalRate=19.47 Time=Wed Jul  1 15:56:11 2020\n",
            "[xla:0](0) Loss=0.00833 Rate=19.59 GlobalRate=19.59 Time=Wed Jul  1 15:56:11 2020\n",
            "[xla:4](100) Loss=0.00553 Rate=56.86 GlobalRate=77.97 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:5](100) Loss=0.00553 Rate=56.33 GlobalRate=78.02 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:2](100) Loss=0.00553 Rate=56.23 GlobalRate=78.21 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:7](100) Loss=0.00553 Rate=56.23 GlobalRate=78.30 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:3](100) Loss=0.00553 Rate=56.22 GlobalRate=78.21 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:6](100) Loss=0.00553 Rate=56.00 GlobalRate=77.85 Time=Wed Jul  1 15:58:50 2020\n",
            "[xla:0](100) Loss=0.00553 Rate=56.04 GlobalRate=77.95 Time=Wed Jul  1 15:58:51 2020\n",
            "[xla:1](100) Loss=0.00553 Rate=56.08 GlobalRate=77.57 Time=Wed Jul  1 15:58:51 2020\n",
            "[xla:5](200) Loss=0.01664 Rate=70.50 GlobalRate=78.97 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:3](200) Loss=0.01664 Rate=70.47 GlobalRate=79.08 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:4](200) Loss=0.01664 Rate=70.69 GlobalRate=78.92 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:2](200) Loss=0.01664 Rate=70.45 GlobalRate=79.05 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:7](200) Loss=0.01664 Rate=70.44 GlobalRate=79.10 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:6](200) Loss=0.01664 Rate=70.53 GlobalRate=79.01 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:1](200) Loss=0.01664 Rate=70.52 GlobalRate=78.83 Time=Wed Jul  1 16:01:30 2020\n",
            "[xla:0](200) Loss=0.01664 Rate=70.36 GlobalRate=78.91 Time=Wed Jul  1 16:01:31 2020\n",
            "[xla:5](300) Loss=0.06194 Rate=75.74 GlobalRate=79.06 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:3](300) Loss=0.06194 Rate=75.73 GlobalRate=79.13 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:2](300) Loss=0.06194 Rate=75.74 GlobalRate=79.12 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:0](300) Loss=0.06194 Rate=75.95 GlobalRate=79.17 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:1](300) Loss=0.06194 Rate=75.88 GlobalRate=79.04 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:7](300) Loss=0.06194 Rate=75.73 GlobalRate=79.15 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:6](300) Loss=0.06194 Rate=75.75 GlobalRate=79.08 Time=Wed Jul  1 16:04:11 2020\n",
            "[xla:4](300) Loss=0.06194 Rate=75.79 GlobalRate=79.01 Time=Wed Jul  1 16:04:11 2020\n",
            "Finished training epoch 149\n",
            "[xla:7] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:1] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:3] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:6] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:2] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:5] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:4] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:0] Accuracy=89.64%\n",
            "Epoch:  150 Loss:  4.801971670240164 Accuracy:  89.63578780680919 % Validation Loss  0\n",
            "[xla:7](0) Loss=0.22146 Rate=19.87 GlobalRate=19.87 Time=Wed Jul  1 16:07:33 2020\n",
            "[xla:1](0) Loss=0.22146 Rate=19.09 GlobalRate=19.09 Time=Wed Jul  1 16:07:33 2020\n",
            "[xla:3](0) Loss=0.22146 Rate=19.17 GlobalRate=19.17 Time=Wed Jul  1 16:07:34 2020\n",
            "[xla:5](0) Loss=0.22146 Rate=18.80 GlobalRate=18.80 Time=Wed Jul  1 16:07:34 2020\n",
            "[xla:6](0) Loss=0.22146 Rate=17.99 GlobalRate=17.99 Time=Wed Jul  1 16:07:34 2020\n",
            "[xla:2](0) Loss=0.22146 Rate=18.43 GlobalRate=18.43 Time=Wed Jul  1 16:07:34 2020\n",
            "[xla:4](0) Loss=0.22146 Rate=17.30 GlobalRate=17.30 Time=Wed Jul  1 16:07:35 2020\n",
            "[xla:0](0) Loss=0.22146 Rate=16.95 GlobalRate=16.95 Time=Wed Jul  1 16:07:35 2020\n",
            "[xla:6](100) Loss=0.03480 Rate=55.09 GlobalRate=77.19 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:1](100) Loss=0.03480 Rate=55.30 GlobalRate=77.03 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:3](100) Loss=0.03480 Rate=55.37 GlobalRate=77.10 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:5](100) Loss=0.03480 Rate=55.40 GlobalRate=77.31 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:7](100) Loss=0.03480 Rate=55.50 GlobalRate=76.98 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:2](100) Loss=0.03480 Rate=55.27 GlobalRate=77.28 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:4](100) Loss=0.03480 Rate=54.92 GlobalRate=77.23 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:0](100) Loss=0.03480 Rate=54.77 GlobalRate=77.14 Time=Wed Jul  1 16:10:15 2020\n",
            "[xla:2](200) Loss=0.00431 Rate=70.31 GlobalRate=78.77 Time=Wed Jul  1 16:12:54 2020\n",
            "[xla:5](200) Loss=0.00431 Rate=70.35 GlobalRate=78.78 Time=Wed Jul  1 16:12:54 2020\n",
            "[xla:3](200) Loss=0.00431 Rate=70.33 GlobalRate=78.66 Time=Wed Jul  1 16:12:54 2020\n",
            "[xla:1](200) Loss=0.00431 Rate=70.20 GlobalRate=78.54 Time=Wed Jul  1 16:12:54 2020\n",
            "[xla:0](200) Loss=0.00431 Rate=70.25 GlobalRate=78.81 Time=Wed Jul  1 16:12:54 2020\n",
            "[xla:6](200) Loss=0.00431 Rate=69.96 GlobalRate=78.50 Time=Wed Jul  1 16:12:55 2020\n",
            "[xla:4](200) Loss=0.00431 Rate=70.02 GlobalRate=78.63 Time=Wed Jul  1 16:12:55 2020\n",
            "[xla:7](200) Loss=0.00431 Rate=70.11 GlobalRate=78.38 Time=Wed Jul  1 16:12:55 2020\n",
            "[xla:2](300) Loss=0.02718 Rate=74.98 GlobalRate=78.55 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:6](300) Loss=0.02718 Rate=75.09 GlobalRate=78.50 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:3](300) Loss=0.02718 Rate=75.00 GlobalRate=78.48 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:7](300) Loss=0.02718 Rate=75.16 GlobalRate=78.43 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:0](300) Loss=0.02718 Rate=75.05 GlobalRate=78.62 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:5](300) Loss=0.02718 Rate=74.99 GlobalRate=78.54 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:4](300) Loss=0.02718 Rate=75.12 GlobalRate=78.59 Time=Wed Jul  1 16:15:38 2020\n",
            "[xla:1](300) Loss=0.02718 Rate=75.01 GlobalRate=78.44 Time=Wed Jul  1 16:15:38 2020\n",
            "Finished training epoch 150\n",
            "[xla:2] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:1] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:7] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:5] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:0] Accuracy=88.47%\n",
            "[xla:4] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:6] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:3] Accuracy=88.47%\n",
            "Epoch:  151 Loss:  3.938532287370207 Accuracy:  88.47189231987332 % Validation Loss  0\n",
            "[xla:2](0) Loss=0.00034 Rate=26.41 GlobalRate=26.41 Time=Wed Jul  1 16:18:55 2020\n",
            "[xla:1](0) Loss=0.00034 Rate=20.10 GlobalRate=20.10 Time=Wed Jul  1 16:18:56 2020\n",
            "[xla:7](0) Loss=0.00034 Rate=19.33 GlobalRate=19.33 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:4](0) Loss=0.00034 Rate=20.06 GlobalRate=20.06 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:3](0) Loss=0.00034 Rate=20.13 GlobalRate=20.13 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:6](0) Loss=0.00034 Rate=19.80 GlobalRate=19.80 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:5](0) Loss=0.00034 Rate=19.36 GlobalRate=19.36 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:0](0) Loss=0.00034 Rate=19.60 GlobalRate=19.60 Time=Wed Jul  1 16:18:57 2020\n",
            "[xla:2](100) Loss=0.00019 Rate=58.38 GlobalRate=78.13 Time=Wed Jul  1 16:21:35 2020\n",
            "[xla:7](100) Loss=0.00019 Rate=56.17 GlobalRate=78.27 Time=Wed Jul  1 16:21:35 2020\n",
            "[xla:6](100) Loss=0.00019 Rate=56.38 GlobalRate=78.38 Time=Wed Jul  1 16:21:35 2020\n",
            "[xla:3](100) Loss=0.00019 Rate=56.49 GlobalRate=78.40 Time=Wed Jul  1 16:21:35 2020\n",
            "[xla:5](100) Loss=0.00019 Rate=56.21 GlobalRate=78.32 Time=Wed Jul  1 16:21:35 2020\n",
            "[xla:1](100) Loss=0.00019 Rate=56.29 GlobalRate=78.09 Time=Wed Jul  1 16:21:36 2020\n",
            "[xla:4](100) Loss=0.00019 Rate=56.23 GlobalRate=78.01 Time=Wed Jul  1 16:21:36 2020\n",
            "[xla:0](100) Loss=0.00019 Rate=56.03 GlobalRate=77.93 Time=Wed Jul  1 16:21:36 2020\n",
            "[xla:6](200) Loss=0.00024 Rate=70.65 GlobalRate=79.26 Time=Wed Jul  1 16:24:15 2020\n",
            "[xla:5](200) Loss=0.00024 Rate=70.57 GlobalRate=79.21 Time=Wed Jul  1 16:24:15 2020\n",
            "[xla:3](200) Loss=0.00024 Rate=70.62 GlobalRate=79.21 Time=Wed Jul  1 16:24:15 2020\n",
            "[xla:0](200) Loss=0.00024 Rate=70.64 GlobalRate=79.13 Time=Wed Jul  1 16:24:16 2020\n",
            "[xla:1](200) Loss=0.00024 Rate=70.50 GlobalRate=79.02 Time=Wed Jul  1 16:24:16 2020\n",
            "[xla:7](200) Loss=0.00024 Rate=70.36 GlobalRate=79.03 Time=Wed Jul  1 16:24:16 2020\n",
            "[xla:2](200) Loss=0.00024 Rate=71.15 GlobalRate=78.88 Time=Wed Jul  1 16:24:16 2020\n",
            "[xla:4](200) Loss=0.00024 Rate=70.48 GlobalRate=78.98 Time=Wed Jul  1 16:24:16 2020\n",
            "[xla:5](300) Loss=0.00012 Rate=75.58 GlobalRate=79.12 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:0](300) Loss=0.00012 Rate=75.75 GlobalRate=79.14 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:3](300) Loss=0.00012 Rate=75.64 GlobalRate=79.14 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:1](300) Loss=0.00012 Rate=75.70 GlobalRate=79.07 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:4](300) Loss=0.00012 Rate=75.85 GlobalRate=79.13 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:7](300) Loss=0.00012 Rate=75.66 GlobalRate=79.09 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:6](300) Loss=0.00012 Rate=75.58 GlobalRate=79.13 Time=Wed Jul  1 16:26:57 2020\n",
            "[xla:2](300) Loss=0.00012 Rate=76.05 GlobalRate=79.03 Time=Wed Jul  1 16:26:57 2020\n",
            "Finished training epoch 151\n",
            "[xla:0] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:7] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:6] Accuracy=89.08%\n",
            "[xla:2] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:3] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:4] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:1] Accuracy=89.08%\n",
            "[xla:5] Accuracy=89.08%\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "Epoch:  152 Loss:  1.871220083743431 Accuracy:  89.08155186064924 % Validation Loss  0\n",
            "[xla:0](0) Loss=0.00806 Rate=24.23 GlobalRate=24.23 Time=Wed Jul  1 16:30:11 2020\n",
            "[xla:6](0) Loss=0.00806 Rate=21.19 GlobalRate=21.19 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:2](0) Loss=0.00806 Rate=20.43 GlobalRate=20.43 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:7](0) Loss=0.00806 Rate=19.63 GlobalRate=19.63 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:3](0) Loss=0.00806 Rate=20.03 GlobalRate=20.03 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:1](0) Loss=0.00806 Rate=20.54 GlobalRate=20.54 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:4](0) Loss=0.00806 Rate=19.65 GlobalRate=19.65 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:5](0) Loss=0.00806 Rate=20.00 GlobalRate=20.00 Time=Wed Jul  1 16:30:12 2020\n",
            "[xla:1](100) Loss=0.00117 Rate=56.01 GlobalRate=77.44 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:5](100) Loss=0.00117 Rate=55.84 GlobalRate=77.45 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:6](100) Loss=0.00117 Rate=56.10 GlobalRate=77.27 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:0](100) Loss=0.00117 Rate=56.99 GlobalRate=77.10 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:3](100) Loss=0.00117 Rate=55.78 GlobalRate=77.34 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:2](100) Loss=0.00117 Rate=55.85 GlobalRate=77.26 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:4](100) Loss=0.00117 Rate=55.66 GlobalRate=77.33 Time=Wed Jul  1 16:32:53 2020\n",
            "[xla:7](100) Loss=0.00117 Rate=55.25 GlobalRate=76.70 Time=Wed Jul  1 16:32:54 2020\n",
            "[xla:2](200) Loss=0.01518 Rate=69.23 GlobalRate=77.70 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:0](200) Loss=0.01518 Rate=69.68 GlobalRate=77.61 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:6](200) Loss=0.01518 Rate=69.31 GlobalRate=77.69 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:1](200) Loss=0.01518 Rate=69.25 GlobalRate=77.76 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:5](200) Loss=0.01518 Rate=69.18 GlobalRate=77.76 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:4](200) Loss=0.01518 Rate=69.12 GlobalRate=77.71 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:3](200) Loss=0.01518 Rate=69.16 GlobalRate=77.70 Time=Wed Jul  1 16:35:37 2020\n",
            "[xla:7](200) Loss=0.01518 Rate=68.98 GlobalRate=77.41 Time=Wed Jul  1 16:35:38 2020\n",
            "[xla:6](300) Loss=0.02081 Rate=74.39 GlobalRate=77.72 Time=Wed Jul  1 16:38:21 2020\n",
            "[xla:1](300) Loss=0.02081 Rate=74.35 GlobalRate=77.76 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:3](300) Loss=0.02081 Rate=74.33 GlobalRate=77.73 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:4](300) Loss=0.02081 Rate=74.31 GlobalRate=77.73 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:5](300) Loss=0.02081 Rate=74.33 GlobalRate=77.76 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:7](300) Loss=0.02081 Rate=74.53 GlobalRate=77.68 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:2](300) Loss=0.02081 Rate=74.32 GlobalRate=77.70 Time=Wed Jul  1 16:38:22 2020\n",
            "[xla:0](300) Loss=0.02081 Rate=74.50 GlobalRate=77.65 Time=Wed Jul  1 16:38:22 2020\n",
            "Finished training epoch 152\n",
            "[xla:5] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:3] Accuracy=88.43%\n",
            "[xla:1] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:4] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:7] Accuracy=88.43%\n",
            "[xla:2] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:6] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:0] Accuracy=88.43%\n",
            "Epoch:  153 Loss:  2.0977563851737457 Accuracy:  88.43230403800474 % Validation Loss  0\n",
            "[xla:5](0) Loss=0.01982 Rate=22.41 GlobalRate=22.41 Time=Wed Jul  1 16:41:38 2020\n",
            "[xla:3](0) Loss=0.01982 Rate=20.73 GlobalRate=20.73 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:1](0) Loss=0.01982 Rate=20.16 GlobalRate=20.16 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:4](0) Loss=0.01982 Rate=19.42 GlobalRate=19.42 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:2](0) Loss=0.01982 Rate=20.29 GlobalRate=20.29 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:7](0) Loss=0.01982 Rate=19.71 GlobalRate=19.70 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:6](0) Loss=0.01982 Rate=19.81 GlobalRate=19.81 Time=Wed Jul  1 16:41:39 2020\n",
            "[xla:0](0) Loss=0.01982 Rate=19.40 GlobalRate=19.40 Time=Wed Jul  1 16:41:40 2020\n",
            "[xla:1](100) Loss=0.01506 Rate=56.03 GlobalRate=77.67 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:5](100) Loss=0.01506 Rate=56.68 GlobalRate=77.58 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:4](100) Loss=0.01506 Rate=55.83 GlobalRate=77.70 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:2](100) Loss=0.01506 Rate=56.19 GlobalRate=77.85 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:6](100) Loss=0.01506 Rate=56.04 GlobalRate=77.84 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:3](100) Loss=0.01506 Rate=56.19 GlobalRate=77.65 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:7](100) Loss=0.01506 Rate=55.99 GlobalRate=77.81 Time=Wed Jul  1 16:44:19 2020\n",
            "[xla:0](100) Loss=0.01506 Rate=55.77 GlobalRate=77.62 Time=Wed Jul  1 16:44:20 2020\n",
            "[xla:2](200) Loss=0.12221 Rate=69.83 GlobalRate=78.38 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:1](200) Loss=0.12221 Rate=69.73 GlobalRate=78.26 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:5](200) Loss=0.12221 Rate=70.00 GlobalRate=78.22 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:7](200) Loss=0.12221 Rate=69.71 GlobalRate=78.33 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:4](200) Loss=0.12221 Rate=69.65 GlobalRate=78.27 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:3](200) Loss=0.12221 Rate=69.78 GlobalRate=78.24 Time=Wed Jul  1 16:47:01 2020\n",
            "[xla:6](200) Loss=0.12221 Rate=69.52 GlobalRate=78.17 Time=Wed Jul  1 16:47:02 2020\n",
            "[xla:0](200) Loss=0.12221 Rate=69.51 GlobalRate=78.14 Time=Wed Jul  1 16:47:02 2020\n",
            "[xla:2](300) Loss=0.00040 Rate=74.82 GlobalRate=78.30 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:7](300) Loss=0.00040 Rate=74.80 GlobalRate=78.29 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:1](300) Loss=0.00040 Rate=74.79 GlobalRate=78.22 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:0](300) Loss=0.00040 Rate=75.00 GlobalRate=78.31 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:4](300) Loss=0.00040 Rate=74.77 GlobalRate=78.24 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:3](300) Loss=0.00040 Rate=74.84 GlobalRate=78.23 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:5](300) Loss=0.00040 Rate=74.90 GlobalRate=78.20 Time=Wed Jul  1 16:49:45 2020\n",
            "[xla:6](300) Loss=0.00040 Rate=74.93 GlobalRate=78.29 Time=Wed Jul  1 16:49:45 2020\n",
            "Finished training epoch 153\n",
            "[xla:3] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:2] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:7] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:1] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:5] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:4] Accuracy=86.94%\n",
            "[xla:6] Accuracy=86.94%\n",
            "[xla:0] Accuracy=86.94%\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "Epoch:  154 Loss:  3.5080777335766373 Accuracy:  86.93586698337292 % Validation Loss  0\n",
            "[xla:3](0) Loss=0.05387 Rate=26.04 GlobalRate=26.04 Time=Wed Jul  1 16:53:00 2020\n",
            "[xla:2](0) Loss=0.05387 Rate=20.28 GlobalRate=20.28 Time=Wed Jul  1 16:53:02 2020\n",
            "[xla:7](0) Loss=0.05387 Rate=19.26 GlobalRate=19.26 Time=Wed Jul  1 16:53:02 2020\n",
            "[xla:4](0) Loss=0.05387 Rate=19.95 GlobalRate=19.95 Time=Wed Jul  1 16:53:02 2020\n",
            "[xla:5](0) Loss=0.05387 Rate=19.63 GlobalRate=19.63 Time=Wed Jul  1 16:53:02 2020\n",
            "[xla:1](0) Loss=0.05387 Rate=18.94 GlobalRate=18.94 Time=Wed Jul  1 16:53:03 2020\n",
            "[xla:0](0) Loss=0.05387 Rate=19.90 GlobalRate=19.90 Time=Wed Jul  1 16:53:03 2020\n",
            "[xla:6](0) Loss=0.05387 Rate=19.30 GlobalRate=19.30 Time=Wed Jul  1 16:53:03 2020\n",
            "[xla:2](100) Loss=0.00313 Rate=55.63 GlobalRate=76.98 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:1](100) Loss=0.00313 Rate=55.24 GlobalRate=77.01 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:3](100) Loss=0.00313 Rate=57.44 GlobalRate=76.84 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:6](100) Loss=0.00313 Rate=55.41 GlobalRate=77.11 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:4](100) Loss=0.00313 Rate=55.59 GlobalRate=77.09 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:7](100) Loss=0.00313 Rate=55.30 GlobalRate=76.95 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:5](100) Loss=0.00313 Rate=55.47 GlobalRate=77.04 Time=Wed Jul  1 16:55:44 2020\n",
            "[xla:0](100) Loss=0.00313 Rate=55.36 GlobalRate=76.74 Time=Wed Jul  1 16:55:45 2020\n",
            "[xla:4](200) Loss=0.00912 Rate=69.36 GlobalRate=77.80 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:6](200) Loss=0.00912 Rate=69.27 GlobalRate=77.80 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:1](200) Loss=0.00912 Rate=69.16 GlobalRate=77.71 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:2](200) Loss=0.00912 Rate=69.30 GlobalRate=77.69 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:5](200) Loss=0.00912 Rate=69.27 GlobalRate=77.74 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:3](200) Loss=0.00912 Rate=70.04 GlobalRate=77.63 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:7](200) Loss=0.00912 Rate=69.21 GlobalRate=77.71 Time=Wed Jul  1 16:58:27 2020\n",
            "[xla:0](200) Loss=0.00912 Rate=69.16 GlobalRate=77.54 Time=Wed Jul  1 16:58:28 2020\n",
            "[xla:5](300) Loss=0.02461 Rate=74.08 GlobalRate=77.59 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:4](300) Loss=0.02461 Rate=74.06 GlobalRate=77.60 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:3](300) Loss=0.02461 Rate=74.38 GlobalRate=77.51 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:7](300) Loss=0.02461 Rate=74.04 GlobalRate=77.56 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:2](300) Loss=0.02461 Rate=74.09 GlobalRate=77.55 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:0](300) Loss=0.02461 Rate=74.32 GlobalRate=77.61 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:6](300) Loss=0.02461 Rate=74.03 GlobalRate=77.60 Time=Wed Jul  1 17:01:13 2020\n",
            "[xla:1](300) Loss=0.02461 Rate=74.01 GlobalRate=77.56 Time=Wed Jul  1 17:01:13 2020\n",
            "Finished training epoch 154\n",
            "[xla:7] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:5] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:1] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:6] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:3] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:2] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:4] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:0] Accuracy=75.49%\n",
            "Epoch:  155 Loss:  2.881725404681547 Accuracy:  75.48693586698337 % Validation Loss  0\n",
            "[xla:7](0) Loss=0.03576 Rate=21.25 GlobalRate=21.25 Time=Wed Jul  1 17:04:37 2020\n",
            "[xla:5](0) Loss=0.03576 Rate=19.24 GlobalRate=19.24 Time=Wed Jul  1 17:04:37 2020\n",
            "[xla:4](0) Loss=0.03576 Rate=20.67 GlobalRate=20.67 Time=Wed Jul  1 17:04:37 2020\n",
            "[xla:1](0) Loss=0.03576 Rate=19.12 GlobalRate=19.12 Time=Wed Jul  1 17:04:37 2020\n",
            "[xla:2](0) Loss=0.03576 Rate=19.78 GlobalRate=19.78 Time=Wed Jul  1 17:04:37 2020\n",
            "[xla:6](0) Loss=0.03576 Rate=19.06 GlobalRate=19.06 Time=Wed Jul  1 17:04:38 2020\n",
            "[xla:3](0) Loss=0.03576 Rate=19.18 GlobalRate=19.18 Time=Wed Jul  1 17:04:38 2020\n",
            "[xla:0](0) Loss=0.03576 Rate=19.42 GlobalRate=19.42 Time=Wed Jul  1 17:04:38 2020\n",
            "[xla:6](100) Loss=0.08169 Rate=54.66 GlobalRate=76.05 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:1](100) Loss=0.08169 Rate=54.65 GlobalRate=76.01 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:3](100) Loss=0.08169 Rate=54.71 GlobalRate=76.07 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:0](100) Loss=0.08169 Rate=54.89 GlobalRate=76.24 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:2](100) Loss=0.08169 Rate=54.91 GlobalRate=76.10 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:7](100) Loss=0.08169 Rate=55.23 GlobalRate=75.88 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:5](100) Loss=0.08169 Rate=54.60 GlobalRate=75.87 Time=Wed Jul  1 17:07:21 2020\n",
            "[xla:4](100) Loss=0.08169 Rate=54.95 GlobalRate=75.73 Time=Wed Jul  1 17:07:22 2020\n",
            "[xla:0](200) Loss=0.01661 Rate=67.63 GlobalRate=76.18 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:6](200) Loss=0.01661 Rate=67.50 GlobalRate=76.05 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:1](200) Loss=0.01661 Rate=67.50 GlobalRate=76.03 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:7](200) Loss=0.01661 Rate=67.74 GlobalRate=75.98 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:3](200) Loss=0.01661 Rate=67.52 GlobalRate=76.06 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:2](200) Loss=0.01661 Rate=67.61 GlobalRate=76.09 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:5](200) Loss=0.01661 Rate=67.43 GlobalRate=75.93 Time=Wed Jul  1 17:10:09 2020\n",
            "[xla:4](200) Loss=0.01661 Rate=67.60 GlobalRate=75.88 Time=Wed Jul  1 17:10:10 2020\n",
            "[xla:6](300) Loss=0.01122 Rate=73.01 GlobalRate=76.26 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:7](300) Loss=0.01122 Rate=73.09 GlobalRate=76.20 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:1](300) Loss=0.01122 Rate=72.98 GlobalRate=76.23 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:5](300) Loss=0.01122 Rate=73.02 GlobalRate=76.20 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:0](300) Loss=0.01122 Rate=73.01 GlobalRate=76.32 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:2](300) Loss=0.01122 Rate=73.02 GlobalRate=76.27 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:3](300) Loss=0.01122 Rate=72.98 GlobalRate=76.25 Time=Wed Jul  1 17:12:56 2020\n",
            "[xla:4](300) Loss=0.01122 Rate=73.31 GlobalRate=76.29 Time=Wed Jul  1 17:12:56 2020\n",
            "Finished training epoch 155\n",
            "[xla:7] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:6] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:4] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:5] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:0] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:2] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:1] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:3] Accuracy=88.97%\n",
            "Epoch:  156 Loss:  4.067733418930198 Accuracy:  88.97070467141727 % Validation Loss  0\n",
            "[xla:7](0) Loss=0.00244 Rate=28.37 GlobalRate=28.37 Time=Wed Jul  1 17:16:13 2020\n",
            "[xla:6](0) Loss=0.00244 Rate=19.94 GlobalRate=19.94 Time=Wed Jul  1 17:16:15 2020\n",
            "[xla:4](0) Loss=0.00244 Rate=19.87 GlobalRate=19.87 Time=Wed Jul  1 17:16:15 2020\n",
            "[xla:5](0) Loss=0.00244 Rate=19.36 GlobalRate=19.36 Time=Wed Jul  1 17:16:15 2020\n",
            "[xla:2](0) Loss=0.00244 Rate=19.63 GlobalRate=19.63 Time=Wed Jul  1 17:16:15 2020\n",
            "[xla:1](0) Loss=0.00244 Rate=18.87 GlobalRate=18.87 Time=Wed Jul  1 17:16:16 2020\n",
            "[xla:0](0) Loss=0.00244 Rate=18.32 GlobalRate=18.32 Time=Wed Jul  1 17:16:16 2020\n",
            "[xla:3](0) Loss=0.00244 Rate=18.56 GlobalRate=18.56 Time=Wed Jul  1 17:16:16 2020\n",
            "[xla:1](100) Loss=0.00362 Rate=54.41 GlobalRate=75.75 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:2](100) Loss=0.00362 Rate=54.59 GlobalRate=75.68 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:7](100) Loss=0.00362 Rate=57.37 GlobalRate=75.43 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:5](100) Loss=0.00362 Rate=54.44 GlobalRate=75.57 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:0](100) Loss=0.00362 Rate=54.20 GlobalRate=75.68 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:6](100) Loss=0.00362 Rate=54.64 GlobalRate=75.59 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:4](100) Loss=0.00362 Rate=54.60 GlobalRate=75.58 Time=Wed Jul  1 17:19:00 2020\n",
            "[xla:3](100) Loss=0.00362 Rate=53.99 GlobalRate=75.24 Time=Wed Jul  1 17:19:01 2020\n",
            "[xla:2](200) Loss=0.07076 Rate=68.44 GlobalRate=76.66 Time=Wed Jul  1 17:21:44 2020\n",
            "[xla:1](200) Loss=0.07076 Rate=68.32 GlobalRate=76.66 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:6](200) Loss=0.07076 Rate=68.42 GlobalRate=76.58 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:5](200) Loss=0.07076 Rate=68.34 GlobalRate=76.57 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:0](200) Loss=0.07076 Rate=68.24 GlobalRate=76.62 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:7](200) Loss=0.07076 Rate=69.51 GlobalRate=76.50 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:4](200) Loss=0.07076 Rate=68.26 GlobalRate=76.46 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:3](200) Loss=0.07076 Rate=68.24 GlobalRate=76.46 Time=Wed Jul  1 17:21:45 2020\n",
            "[xla:0](300) Loss=0.00937 Rate=73.35 GlobalRate=76.67 Time=Wed Jul  1 17:24:31 2020\n",
            "[xla:6](300) Loss=0.00937 Rate=73.42 GlobalRate=76.64 Time=Wed Jul  1 17:24:31 2020\n",
            "[xla:1](300) Loss=0.00937 Rate=73.34 GlobalRate=76.67 Time=Wed Jul  1 17:24:31 2020\n",
            "[xla:7](300) Loss=0.00937 Rate=73.81 GlobalRate=76.56 Time=Wed Jul  1 17:24:32 2020\n",
            "[xla:2](300) Loss=0.00937 Rate=73.34 GlobalRate=76.64 Time=Wed Jul  1 17:24:32 2020\n",
            "[xla:5](300) Loss=0.00937 Rate=73.35 GlobalRate=76.61 Time=Wed Jul  1 17:24:32 2020\n",
            "[xla:3](300) Loss=0.00937 Rate=73.54 GlobalRate=76.66 Time=Wed Jul  1 17:24:32 2020\n",
            "[xla:4](300) Loss=0.00937 Rate=73.45 GlobalRate=76.61 Time=Wed Jul  1 17:24:32 2020\n",
            "Finished training epoch 156\n",
            "[xla:5] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:4] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:0] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:1] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:7] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:2] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:6] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:3] Accuracy=80.39%\n",
            "Epoch:  157 Loss:  3.1491340452271244 Accuracy:  80.38796516231196 % Validation Loss  0\n",
            "[xla:5](0) Loss=0.03890 Rate=24.53 GlobalRate=24.53 Time=Wed Jul  1 17:27:55 2020\n",
            "[xla:4](0) Loss=0.03890 Rate=21.29 GlobalRate=21.29 Time=Wed Jul  1 17:27:56 2020\n",
            "[xla:2](0) Loss=0.03890 Rate=20.10 GlobalRate=20.10 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:0](0) Loss=0.03890 Rate=18.82 GlobalRate=18.82 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:7](0) Loss=0.03890 Rate=19.24 GlobalRate=19.24 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:1](0) Loss=0.03890 Rate=18.86 GlobalRate=18.86 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:3](0) Loss=0.03890 Rate=19.37 GlobalRate=19.37 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:6](0) Loss=0.03890 Rate=18.97 GlobalRate=18.97 Time=Wed Jul  1 17:27:57 2020\n",
            "[xla:4](100) Loss=0.00379 Rate=55.13 GlobalRate=75.70 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:5](100) Loss=0.00379 Rate=56.14 GlobalRate=75.60 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:1](100) Loss=0.00379 Rate=54.50 GlobalRate=75.89 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:6](100) Loss=0.00379 Rate=54.58 GlobalRate=75.96 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:0](100) Loss=0.00379 Rate=54.41 GlobalRate=75.78 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:3](100) Loss=0.00379 Rate=54.72 GlobalRate=76.01 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:2](100) Loss=0.00379 Rate=54.77 GlobalRate=75.73 Time=Wed Jul  1 17:30:41 2020\n",
            "[xla:7](100) Loss=0.00379 Rate=54.39 GlobalRate=75.54 Time=Wed Jul  1 17:30:42 2020\n",
            "[xla:0](200) Loss=0.00330 Rate=68.03 GlobalRate=76.43 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:1](200) Loss=0.00330 Rate=68.05 GlobalRate=76.48 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:3](200) Loss=0.00330 Rate=68.14 GlobalRate=76.54 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:4](200) Loss=0.00330 Rate=68.27 GlobalRate=76.36 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:6](200) Loss=0.00330 Rate=68.06 GlobalRate=76.50 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:5](200) Loss=0.00330 Rate=68.68 GlobalRate=76.31 Time=Wed Jul  1 17:33:27 2020\n",
            "[xla:7](200) Loss=0.00330 Rate=67.92 GlobalRate=76.23 Time=Wed Jul  1 17:33:28 2020\n",
            "[xla:2](200) Loss=0.00330 Rate=67.97 GlobalRate=76.24 Time=Wed Jul  1 17:33:28 2020\n",
            "[xla:0](300) Loss=0.00214 Rate=72.72 GlobalRate=76.24 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:1](300) Loss=0.00214 Rate=72.72 GlobalRate=76.26 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:3](300) Loss=0.00214 Rate=72.76 GlobalRate=76.31 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:4](300) Loss=0.00214 Rate=72.83 GlobalRate=76.20 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:7](300) Loss=0.00214 Rate=72.98 GlobalRate=76.27 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:2](300) Loss=0.00214 Rate=73.01 GlobalRate=76.28 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:6](300) Loss=0.00214 Rate=72.74 GlobalRate=76.29 Time=Wed Jul  1 17:36:16 2020\n",
            "[xla:5](300) Loss=0.00214 Rate=72.99 GlobalRate=76.16 Time=Wed Jul  1 17:36:16 2020\n",
            "Finished training epoch 157\n",
            "[xla:7] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:2] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:6] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:1] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:4] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:5] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:3] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:0] Accuracy=86.44%\n",
            "Epoch:  158 Loss:  3.5656114157095113 Accuracy:  86.43705463182899 % Validation Loss  0\n",
            "[xla:7](0) Loss=0.06295 Rate=24.75 GlobalRate=24.75 Time=Wed Jul  1 17:39:36 2020\n",
            "[xla:2](0) Loss=0.06295 Rate=21.99 GlobalRate=21.99 Time=Wed Jul  1 17:39:37 2020\n",
            "[xla:4](0) Loss=0.06295 Rate=20.25 GlobalRate=20.25 Time=Wed Jul  1 17:39:38 2020\n",
            "[xla:6](0) Loss=0.06295 Rate=19.12 GlobalRate=19.12 Time=Wed Jul  1 17:39:38 2020\n",
            "[xla:1](0) Loss=0.06295 Rate=18.32 GlobalRate=18.32 Time=Wed Jul  1 17:39:38 2020\n",
            "[xla:5](0) Loss=0.06295 Rate=18.55 GlobalRate=18.55 Time=Wed Jul  1 17:39:39 2020\n",
            "[xla:3](0) Loss=0.06295 Rate=18.36 GlobalRate=18.36 Time=Wed Jul  1 17:39:39 2020\n",
            "[xla:0](0) Loss=0.06295 Rate=18.37 GlobalRate=18.37 Time=Wed Jul  1 17:39:39 2020\n",
            "[xla:0](100) Loss=0.00626 Rate=53.96 GlobalRate=75.28 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:7](100) Loss=0.00626 Rate=55.73 GlobalRate=74.84 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:2](100) Loss=0.00626 Rate=54.85 GlobalRate=74.91 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:3](100) Loss=0.00626 Rate=53.87 GlobalRate=75.15 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:4](100) Loss=0.00626 Rate=54.42 GlobalRate=75.12 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:1](100) Loss=0.00626 Rate=53.80 GlobalRate=75.05 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:6](100) Loss=0.00626 Rate=53.85 GlobalRate=74.76 Time=Wed Jul  1 17:42:24 2020\n",
            "[xla:5](100) Loss=0.00626 Rate=53.60 GlobalRate=74.64 Time=Wed Jul  1 17:42:25 2020\n",
            "[xla:1](200) Loss=0.02619 Rate=67.67 GlobalRate=75.97 Time=Wed Jul  1 17:45:10 2020\n",
            "[xla:0](200) Loss=0.02619 Rate=67.71 GlobalRate=76.06 Time=Wed Jul  1 17:45:10 2020\n",
            "[xla:3](200) Loss=0.02619 Rate=67.68 GlobalRate=76.00 Time=Wed Jul  1 17:45:10 2020\n",
            "[xla:4](200) Loss=0.02619 Rate=67.90 GlobalRate=75.98 Time=Wed Jul  1 17:45:10 2020\n",
            "[xla:7](200) Loss=0.02619 Rate=68.40 GlobalRate=75.82 Time=Wed Jul  1 17:45:10 2020\n",
            "[xla:6](200) Loss=0.02619 Rate=67.76 GlobalRate=75.87 Time=Wed Jul  1 17:45:11 2020\n",
            "[xla:2](200) Loss=0.02619 Rate=67.82 GlobalRate=75.68 Time=Wed Jul  1 17:45:11 2020\n",
            "[xla:5](200) Loss=0.02619 Rate=67.50 GlobalRate=75.69 Time=Wed Jul  1 17:45:12 2020\n",
            "[xla:1](300) Loss=0.11836 Rate=71.98 GlobalRate=75.59 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:3](300) Loss=0.11836 Rate=71.99 GlobalRate=75.62 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:6](300) Loss=0.11836 Rate=72.12 GlobalRate=75.59 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:7](300) Loss=0.11836 Rate=72.27 GlobalRate=75.50 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:2](300) Loss=0.11836 Rate=72.26 GlobalRate=75.52 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:0](300) Loss=0.11836 Rate=71.98 GlobalRate=75.65 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:4](300) Loss=0.11836 Rate=72.07 GlobalRate=75.61 Time=Wed Jul  1 17:48:01 2020\n",
            "[xla:5](300) Loss=0.11836 Rate=72.27 GlobalRate=75.61 Time=Wed Jul  1 17:48:01 2020\n",
            "Finished training epoch 158\n",
            "[xla:1] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:3] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:2] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:4] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:5] Accuracy=86.46%\n",
            "[xla:0] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:6] Accuracy=86.46%\n",
            "[xla:7] Accuracy=86.46%\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "Epoch:  159 Loss:  2.0287334699739614 Accuracy:  86.46080760095012 % Validation Loss  0\n",
            "[xla:1](0) Loss=0.00116 Rate=22.42 GlobalRate=22.42 Time=Wed Jul  1 17:51:26 2020\n",
            "[xla:3](0) Loss=0.00116 Rate=20.49 GlobalRate=20.49 Time=Wed Jul  1 17:51:27 2020\n",
            "[xla:4](0) Loss=0.00116 Rate=19.62 GlobalRate=19.62 Time=Wed Jul  1 17:51:28 2020\n",
            "[xla:2](0) Loss=0.00116 Rate=19.15 GlobalRate=19.15 Time=Wed Jul  1 17:51:28 2020\n",
            "[xla:7](0) Loss=0.00116 Rate=18.58 GlobalRate=18.58 Time=Wed Jul  1 17:51:28 2020\n",
            "[xla:6](0) Loss=0.00116 Rate=18.45 GlobalRate=18.45 Time=Wed Jul  1 17:51:28 2020\n",
            "[xla:0](0) Loss=0.00116 Rate=17.83 GlobalRate=17.83 Time=Wed Jul  1 17:51:28 2020\n",
            "[xla:5](0) Loss=0.00116 Rate=17.98 GlobalRate=17.98 Time=Wed Jul  1 17:51:29 2020\n",
            "[xla:4](100) Loss=0.00227 Rate=54.40 GlobalRate=75.38 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:6](100) Loss=0.00227 Rate=54.17 GlobalRate=75.57 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:7](100) Loss=0.00227 Rate=54.22 GlobalRate=75.59 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:5](100) Loss=0.00227 Rate=53.98 GlobalRate=75.48 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:1](100) Loss=0.00227 Rate=55.16 GlobalRate=75.17 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:2](100) Loss=0.00227 Rate=54.13 GlobalRate=75.18 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:0](100) Loss=0.00227 Rate=53.73 GlobalRate=75.16 Time=Wed Jul  1 17:54:13 2020\n",
            "[xla:3](100) Loss=0.00227 Rate=54.31 GlobalRate=74.82 Time=Wed Jul  1 17:54:14 2020\n",
            "[xla:2](200) Loss=0.00200 Rate=67.59 GlobalRate=75.87 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:6](200) Loss=0.00200 Rate=67.49 GlobalRate=75.96 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:7](200) Loss=0.00200 Rate=67.50 GlobalRate=75.97 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:5](200) Loss=0.00200 Rate=67.42 GlobalRate=75.93 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:1](200) Loss=0.00200 Rate=67.89 GlobalRate=75.77 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:0](200) Loss=0.00200 Rate=67.49 GlobalRate=75.90 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:4](200) Loss=0.00200 Rate=67.56 GlobalRate=75.86 Time=Wed Jul  1 17:57:00 2020\n",
            "[xla:3](200) Loss=0.00200 Rate=67.48 GlobalRate=75.53 Time=Wed Jul  1 17:57:02 2020\n",
            "[xla:2](300) Loss=0.00011 Rate=72.03 GlobalRate=75.57 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:4](300) Loss=0.00011 Rate=72.03 GlobalRate=75.57 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:5](300) Loss=0.00011 Rate=71.95 GlobalRate=75.61 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:1](300) Loss=0.00011 Rate=72.14 GlobalRate=75.50 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:3](300) Loss=0.00011 Rate=72.31 GlobalRate=75.53 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:7](300) Loss=0.00011 Rate=71.97 GlobalRate=75.63 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:0](300) Loss=0.00011 Rate=71.97 GlobalRate=75.59 Time=Wed Jul  1 17:59:51 2020\n",
            "[xla:6](300) Loss=0.00011 Rate=71.96 GlobalRate=75.62 Time=Wed Jul  1 17:59:51 2020\n",
            "Finished training epoch 159\n",
            "[xla:2] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:0] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:7] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:5] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:1] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:6] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:3] Accuracy=88.67%\n",
            "[xla:4] Accuracy=88.67%\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "Epoch:  160 Loss:  2.326948538064617 Accuracy:  88.66983372921615 % Validation Loss  0\n",
            "[xla:2](0) Loss=0.00415 Rate=26.49 GlobalRate=26.49 Time=Wed Jul  1 18:03:13 2020\n",
            "[xla:0](0) Loss=0.00415 Rate=21.59 GlobalRate=21.59 Time=Wed Jul  1 18:03:14 2020\n",
            "[xla:1](0) Loss=0.00415 Rate=19.34 GlobalRate=19.34 Time=Wed Jul  1 18:03:15 2020\n",
            "[xla:7](0) Loss=0.00415 Rate=18.88 GlobalRate=18.88 Time=Wed Jul  1 18:03:15 2020\n",
            "[xla:5](0) Loss=0.00415 Rate=18.28 GlobalRate=18.28 Time=Wed Jul  1 18:03:16 2020\n",
            "[xla:6](0) Loss=0.00415 Rate=18.25 GlobalRate=18.25 Time=Wed Jul  1 18:03:16 2020\n",
            "[xla:3](0) Loss=0.00415 Rate=18.51 GlobalRate=18.51 Time=Wed Jul  1 18:03:16 2020\n",
            "[xla:4](0) Loss=0.00415 Rate=18.41 GlobalRate=18.41 Time=Wed Jul  1 18:03:16 2020\n",
            "[xla:2](100) Loss=0.03667 Rate=56.07 GlobalRate=74.42 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:0](100) Loss=0.03667 Rate=54.44 GlobalRate=74.47 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:1](100) Loss=0.03667 Rate=53.90 GlobalRate=74.73 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:4](100) Loss=0.03667 Rate=53.66 GlobalRate=74.80 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:6](100) Loss=0.03667 Rate=53.57 GlobalRate=74.73 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:3](100) Loss=0.03667 Rate=53.55 GlobalRate=74.58 Time=Wed Jul  1 18:06:02 2020\n",
            "[xla:7](100) Loss=0.03667 Rate=53.47 GlobalRate=74.28 Time=Wed Jul  1 18:06:03 2020\n",
            "[xla:5](100) Loss=0.03667 Rate=53.25 GlobalRate=74.22 Time=Wed Jul  1 18:06:03 2020\n",
            "[xla:1](200) Loss=0.03211 Rate=67.55 GlobalRate=75.67 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:6](200) Loss=0.03211 Rate=67.43 GlobalRate=75.68 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:4](200) Loss=0.03211 Rate=67.45 GlobalRate=75.71 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:2](200) Loss=0.03211 Rate=68.37 GlobalRate=75.47 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:3](200) Loss=0.03211 Rate=67.53 GlobalRate=75.69 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:0](200) Loss=0.03211 Rate=67.73 GlobalRate=75.51 Time=Wed Jul  1 18:08:49 2020\n",
            "[xla:7](200) Loss=0.03211 Rate=67.34 GlobalRate=75.41 Time=Wed Jul  1 18:08:50 2020\n",
            "[xla:5](200) Loss=0.03211 Rate=67.30 GlobalRate=75.41 Time=Wed Jul  1 18:08:50 2020\n",
            "[xla:4](300) Loss=0.00140 Rate=72.11 GlobalRate=75.55 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:3](300) Loss=0.00140 Rate=72.15 GlobalRate=75.54 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:6](300) Loss=0.00140 Rate=72.09 GlobalRate=75.52 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:2](300) Loss=0.00140 Rate=72.48 GlobalRate=75.39 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:1](300) Loss=0.00140 Rate=72.13 GlobalRate=75.51 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:0](300) Loss=0.00140 Rate=72.23 GlobalRate=75.42 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:7](300) Loss=0.00140 Rate=72.31 GlobalRate=75.48 Time=Wed Jul  1 18:11:39 2020\n",
            "[xla:5](300) Loss=0.00140 Rate=72.30 GlobalRate=75.49 Time=Wed Jul  1 18:11:39 2020\n",
            "Finished training epoch 160\n",
            "[xla:6] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:5] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:1] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:3] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:7] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:4] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:2] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:0] Accuracy=88.61%\n",
            "Epoch:  161 Loss:  2.434617437865185 Accuracy:  88.60649247822644 % Validation Loss  0\n",
            "[xla:6](0) Loss=0.01811 Rate=23.10 GlobalRate=23.10 Time=Wed Jul  1 18:15:00 2020\n",
            "[xla:1](0) Loss=0.01811 Rate=19.58 GlobalRate=19.58 Time=Wed Jul  1 18:15:01 2020\n",
            "[xla:5](0) Loss=0.01811 Rate=18.55 GlobalRate=18.55 Time=Wed Jul  1 18:15:01 2020\n",
            "[xla:4](0) Loss=0.01811 Rate=19.43 GlobalRate=19.43 Time=Wed Jul  1 18:15:01 2020\n",
            "[xla:3](0) Loss=0.01811 Rate=18.68 GlobalRate=18.68 Time=Wed Jul  1 18:15:02 2020\n",
            "[xla:7](0) Loss=0.01811 Rate=18.97 GlobalRate=18.97 Time=Wed Jul  1 18:15:02 2020\n",
            "[xla:2](0) Loss=0.01811 Rate=19.41 GlobalRate=19.41 Time=Wed Jul  1 18:15:02 2020\n",
            "[xla:0](0) Loss=0.01811 Rate=17.89 GlobalRate=17.89 Time=Wed Jul  1 18:15:02 2020\n",
            "[xla:6](100) Loss=0.00416 Rate=54.93 GlobalRate=74.46 Time=Wed Jul  1 18:17:48 2020\n",
            "[xla:4](100) Loss=0.00416 Rate=53.89 GlobalRate=74.68 Time=Wed Jul  1 18:17:48 2020\n",
            "[xla:7](100) Loss=0.00416 Rate=53.76 GlobalRate=74.70 Time=Wed Jul  1 18:17:48 2020\n",
            "[xla:0](100) Loss=0.00416 Rate=53.52 GlobalRate=74.81 Time=Wed Jul  1 18:17:48 2020\n",
            "[xla:3](100) Loss=0.00416 Rate=53.64 GlobalRate=74.65 Time=Wed Jul  1 18:17:48 2020\n",
            "[xla:1](100) Loss=0.00416 Rate=53.62 GlobalRate=74.19 Time=Wed Jul  1 18:17:49 2020\n",
            "[xla:2](100) Loss=0.00416 Rate=53.73 GlobalRate=74.43 Time=Wed Jul  1 18:17:49 2020\n",
            "[xla:5](100) Loss=0.00416 Rate=53.27 GlobalRate=74.13 Time=Wed Jul  1 18:17:49 2020\n",
            "[xla:2](200) Loss=0.00092 Rate=67.80 GlobalRate=75.77 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:6](200) Loss=0.00092 Rate=68.04 GlobalRate=75.60 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:7](200) Loss=0.00092 Rate=67.58 GlobalRate=75.72 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:0](200) Loss=0.00092 Rate=67.48 GlobalRate=75.78 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:3](200) Loss=0.00092 Rate=67.53 GlobalRate=75.69 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:4](200) Loss=0.00092 Rate=67.62 GlobalRate=75.71 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:5](200) Loss=0.00092 Rate=67.52 GlobalRate=75.54 Time=Wed Jul  1 18:20:35 2020\n",
            "[xla:1](200) Loss=0.00092 Rate=67.42 GlobalRate=75.37 Time=Wed Jul  1 18:20:36 2020\n",
            "[xla:2](300) Loss=0.00083 Rate=72.77 GlobalRate=75.88 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:6](300) Loss=0.00083 Rate=72.87 GlobalRate=75.76 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:7](300) Loss=0.00083 Rate=72.68 GlobalRate=75.84 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:0](300) Loss=0.00083 Rate=72.64 GlobalRate=75.88 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:4](300) Loss=0.00083 Rate=72.70 GlobalRate=75.83 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:3](300) Loss=0.00083 Rate=72.66 GlobalRate=75.82 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:1](300) Loss=0.00083 Rate=72.93 GlobalRate=75.78 Time=Wed Jul  1 18:23:23 2020\n",
            "[xla:5](300) Loss=0.00083 Rate=72.75 GlobalRate=75.77 Time=Wed Jul  1 18:23:23 2020\n",
            "Finished training epoch 161\n",
            "[xla:4] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:5] Accuracy=89.68%\n",
            "[xla:1] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:6] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:3] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:0] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:7] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:2] Accuracy=89.68%\n",
            "Epoch:  162 Loss:  2.978009675984371 Accuracy:  89.67537608867775 % Validation Loss  0\n",
            "[xla:4](0) Loss=0.00206 Rate=22.65 GlobalRate=22.65 Time=Wed Jul  1 18:26:41 2020\n",
            "[xla:7](0) Loss=0.00206 Rate=19.68 GlobalRate=19.68 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:2](0) Loss=0.00206 Rate=19.87 GlobalRate=19.87 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:5](0) Loss=0.00206 Rate=18.96 GlobalRate=18.96 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:6](0) Loss=0.00206 Rate=18.99 GlobalRate=18.99 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:1](0) Loss=0.00206 Rate=18.39 GlobalRate=18.39 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:0](0) Loss=0.00206 Rate=18.64 GlobalRate=18.64 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:3](0) Loss=0.00206 Rate=18.22 GlobalRate=18.22 Time=Wed Jul  1 18:26:42 2020\n",
            "[xla:3](100) Loss=0.02183 Rate=54.10 GlobalRate=75.57 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:1](100) Loss=0.02183 Rate=54.12 GlobalRate=75.51 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:4](100) Loss=0.02183 Rate=55.39 GlobalRate=75.41 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:7](100) Loss=0.02183 Rate=54.53 GlobalRate=75.55 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:2](100) Loss=0.02183 Rate=54.63 GlobalRate=75.62 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:5](100) Loss=0.02183 Rate=54.25 GlobalRate=75.46 Time=Wed Jul  1 18:29:26 2020\n",
            "[xla:6](100) Loss=0.02183 Rate=54.25 GlobalRate=75.44 Time=Wed Jul  1 18:29:27 2020\n",
            "[xla:0](100) Loss=0.02183 Rate=54.00 GlobalRate=75.22 Time=Wed Jul  1 18:29:27 2020\n",
            "[xla:3](200) Loss=0.01110 Rate=67.75 GlobalRate=76.20 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:7](200) Loss=0.01110 Rate=67.94 GlobalRate=76.21 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:6](200) Loss=0.01110 Rate=67.88 GlobalRate=76.20 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:2](200) Loss=0.01110 Rate=67.97 GlobalRate=76.24 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:1](200) Loss=0.01110 Rate=67.75 GlobalRate=76.16 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:4](200) Loss=0.01110 Rate=68.26 GlobalRate=76.12 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:5](200) Loss=0.01110 Rate=67.71 GlobalRate=76.06 Time=Wed Jul  1 18:32:13 2020\n",
            "[xla:0](200) Loss=0.01110 Rate=67.64 GlobalRate=75.97 Time=Wed Jul  1 18:32:14 2020\n",
            "[xla:3](300) Loss=0.16923 Rate=72.58 GlobalRate=76.07 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:1](300) Loss=0.16923 Rate=72.60 GlobalRate=76.05 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:0](300) Loss=0.16923 Rate=72.85 GlobalRate=76.09 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:5](300) Loss=0.16923 Rate=72.70 GlobalRate=76.05 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:2](300) Loss=0.16923 Rate=72.67 GlobalRate=76.09 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:4](300) Loss=0.16923 Rate=72.80 GlobalRate=76.02 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:7](300) Loss=0.16923 Rate=72.65 GlobalRate=76.07 Time=Wed Jul  1 18:35:02 2020\n",
            "[xla:6](300) Loss=0.16923 Rate=72.64 GlobalRate=76.07 Time=Wed Jul  1 18:35:02 2020\n",
            "Finished training epoch 162\n",
            "[xla:3] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:1] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:6] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:7] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:2] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:4] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:5] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:0] Accuracy=64.68%\n",
            "Epoch:  163 Loss:  2.2574682645181174 Accuracy:  64.6793349168646 % Validation Loss  0\n",
            "[xla:3](0) Loss=0.07652 Rate=23.18 GlobalRate=23.18 Time=Wed Jul  1 18:38:25 2020\n",
            "[xla:1](0) Loss=0.07652 Rate=19.91 GlobalRate=19.91 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:7](0) Loss=0.07652 Rate=19.84 GlobalRate=19.84 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:4](0) Loss=0.07652 Rate=19.67 GlobalRate=19.67 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:5](0) Loss=0.07652 Rate=19.56 GlobalRate=19.56 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:6](0) Loss=0.07652 Rate=18.70 GlobalRate=18.70 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:2](0) Loss=0.07652 Rate=18.86 GlobalRate=18.86 Time=Wed Jul  1 18:38:27 2020\n",
            "[xla:0](0) Loss=0.07652 Rate=18.88 GlobalRate=18.88 Time=Wed Jul  1 18:38:28 2020\n",
            "[xla:4](100) Loss=0.00928 Rate=54.18 GlobalRate=75.01 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:3](100) Loss=0.00928 Rate=55.14 GlobalRate=74.75 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:6](100) Loss=0.00928 Rate=53.81 GlobalRate=74.90 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:2](100) Loss=0.00928 Rate=53.86 GlobalRate=74.91 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:5](100) Loss=0.00928 Rate=54.13 GlobalRate=75.00 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:7](100) Loss=0.00928 Rate=54.06 GlobalRate=74.74 Time=Wed Jul  1 18:41:13 2020\n",
            "[xla:1](100) Loss=0.00928 Rate=53.95 GlobalRate=74.54 Time=Wed Jul  1 18:41:14 2020\n",
            "[xla:0](100) Loss=0.00928 Rate=53.70 GlobalRate=74.64 Time=Wed Jul  1 18:41:14 2020\n",
            "[xla:2](200) Loss=0.01437 Rate=67.77 GlobalRate=75.95 Time=Wed Jul  1 18:43:59 2020\n",
            "[xla:4](200) Loss=0.01437 Rate=67.84 GlobalRate=75.96 Time=Wed Jul  1 18:43:59 2020\n",
            "[xla:3](200) Loss=0.01437 Rate=68.24 GlobalRate=75.84 Time=Wed Jul  1 18:43:59 2020\n",
            "[xla:5](200) Loss=0.01437 Rate=67.85 GlobalRate=75.98 Time=Wed Jul  1 18:43:59 2020\n",
            "[xla:6](200) Loss=0.01437 Rate=67.71 GlobalRate=75.92 Time=Wed Jul  1 18:43:59 2020\n",
            "[xla:7](200) Loss=0.01437 Rate=67.64 GlobalRate=75.70 Time=Wed Jul  1 18:44:00 2020\n",
            "[xla:0](200) Loss=0.01437 Rate=67.68 GlobalRate=75.80 Time=Wed Jul  1 18:44:00 2020\n",
            "[xla:1](200) Loss=0.01437 Rate=67.64 GlobalRate=75.63 Time=Wed Jul  1 18:44:00 2020\n",
            "[xla:6](300) Loss=0.00065 Rate=72.57 GlobalRate=75.88 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:4](300) Loss=0.00065 Rate=72.61 GlobalRate=75.91 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:1](300) Loss=0.00065 Rate=72.82 GlobalRate=75.85 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:3](300) Loss=0.00065 Rate=72.75 GlobalRate=75.81 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:2](300) Loss=0.00065 Rate=72.55 GlobalRate=75.88 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:0](300) Loss=0.00065 Rate=72.82 GlobalRate=75.95 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:5](300) Loss=0.00065 Rate=72.60 GlobalRate=75.91 Time=Wed Jul  1 18:46:48 2020\n",
            "[xla:7](300) Loss=0.00065 Rate=72.80 GlobalRate=75.88 Time=Wed Jul  1 18:46:48 2020\n",
            "Finished training epoch 163\n",
            "[xla:3] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:7] Accuracy=75.36%\n",
            "[xla:6] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:2] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:4] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:5] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:1] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:0] Accuracy=75.36%\n",
            "Epoch:  164 Loss:  3.663971911833685 Accuracy:  75.36025336500396 % Validation Loss  0\n",
            "[xla:6](0) Loss=0.00136 Rate=20.27 GlobalRate=20.27 Time=Wed Jul  1 18:50:08 2020\n",
            "[xla:3](0) Loss=0.00136 Rate=20.14 GlobalRate=20.14 Time=Wed Jul  1 18:50:08 2020\n",
            "[xla:7](0) Loss=0.00136 Rate=19.27 GlobalRate=19.27 Time=Wed Jul  1 18:50:08 2020\n",
            "[xla:4](0) Loss=0.00136 Rate=19.76 GlobalRate=19.76 Time=Wed Jul  1 18:50:08 2020\n",
            "[xla:2](0) Loss=0.00136 Rate=19.35 GlobalRate=19.35 Time=Wed Jul  1 18:50:08 2020\n",
            "[xla:1](0) Loss=0.00136 Rate=19.24 GlobalRate=19.24 Time=Wed Jul  1 18:50:09 2020\n",
            "[xla:5](0) Loss=0.00136 Rate=18.99 GlobalRate=18.99 Time=Wed Jul  1 18:50:09 2020\n",
            "[xla:0](0) Loss=0.00136 Rate=18.21 GlobalRate=18.21 Time=Wed Jul  1 18:50:09 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_LUOrjaA8AB",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsWqIix32bLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = float(validate(teacher,test_set))\n",
        "print('Accuracy = ',accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peEfsJ7OChvS",
        "colab_type": "text"
      },
      "source": [
        "# Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seU8hwRxCRRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(teacher,\"./saved_models/trained_teacher_model_final_tpu.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}